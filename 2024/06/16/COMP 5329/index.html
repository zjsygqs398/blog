<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>USYD - COMP 5329 - Hexo</title><link rel="manifest" href="/blog/manifest.json"><meta name="application-name" content="Quansui&#039;s Blog"><meta name="msapplication-TileImage" content="/my_img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Quansui&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="COMP 5329Lecture 2P9 XOR P31 Activation P36 CE P37 KL, Entropy P38 SoftMax p36, p37  two attributes cross-entropy loss issue?the disadvantage of batch gradient descent is when N is too large, the comp"><meta property="og:type" content="blog"><meta property="og:title" content="USYD - COMP 5329"><meta property="og:url" content="https://zjsygqs398.github.io/blog/2024/06/16/COMP%205329/"><meta property="og:site_name" content="Hexo"><meta property="og:description" content="COMP 5329Lecture 2P9 XOR P31 Activation P36 CE P37 KL, Entropy P38 SoftMax p36, p37  two attributes cross-entropy loss issue?the disadvantage of batch gradient descent is when N is too large, the comp"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://zjsygqs398.github.io/blog/img/og_image.png"><meta property="article:published_time" content="2024-06-16T00:10:20.000Z"><meta property="article:modified_time" content="2024-06-20T03:45:02.230Z"><meta property="article:author" content="John Doe"><meta property="article:tag" content="Pytorch"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Deep Learning"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://zjsygqs398.github.io/blog/img/og_image.png"><meta property="fb:app_id"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://zjsygqs398.github.io/blog/2024/06/16/COMP%205329/"},"headline":"USYD - COMP 5329","image":["https://zjsygqs398.github.io/blog/img/og_image.png"],"datePublished":"2024-06-16T00:10:20.000Z","dateModified":"2024-06-20T03:45:02.230Z","author":{"@type":"Person","name":"John Doe"},"publisher":{"@type":"Organization","name":"Hexo","logo":{"@type":"ImageObject","url":"https://zjsygqs398.github.io/my_img/logo.svg"}},"description":"COMP 5329Lecture 2P9 XOR P31 Activation P36 CE P37 KL, Entropy P38 SoftMax p36, p37  two attributes cross-entropy loss issue?the disadvantage of batch gradient descent is when N is too large, the comp"}</script><link rel="canonical" href="https://zjsygqs398.github.io/blog/2024/06/16/COMP%205329/"><link rel="icon" href="/blog/my_img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/blog/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/blog/"><img src="/blog/my_img/logo.svg" alt="Hexo" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/blog/">Home</a><a class="navbar-item" href="/blog/archives">Archives</a><a class="navbar-item" href="/blog/categories">Categories</a><a class="navbar-item" href="/blog/tags">Tags</a><a class="navbar-item" target="_blank" rel="noopener" href="https://github.com/zjsygqs398">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/zjsygqs398"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-06-16T00:10:20.000Z" title="6/16/2024, 10:10:20 AM">2024-06-16</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-06-20T03:45:02.230Z" title="6/20/2024, 1:45:02 PM">2024-06-20</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/USYD-Lecture/">USYD - Lecture</a></span></div></div><h1 class="title is-3 is-size-4-mobile">USYD - COMP 5329</h1><div class="content"><h1 id="COMP-5329"><a href="#COMP-5329" class="headerlink" title="COMP 5329"></a>COMP 5329</h1><h1 id="Lecture-2"><a href="#Lecture-2" class="headerlink" title="Lecture 2"></a>Lecture 2</h1><p>P9 XOR</p>
<p>P31 Activation</p>
<p>P36 CE</p>
<p>P37 KL, Entropy</p>
<p>P38 SoftMax</p>
<p>p36, p37  two attributes cross-entropy loss issue?<br>the disadvantage of batch gradient descent is when N is too large, the computation is very expensive, but the one example SGD only uses the single example. It may not be the best, due to the random.</p>
<p>mini-batches SGD: divide into mini-batches, in each epoch calculate the gradient</p>
<p>extension content:</p>
<p>sensitivity? make the back propagation similar to the feedforward</p>
<h1 id="Lecture-3-GD"><a href="#Lecture-3-GD" class="headerlink" title="Lecture 3  GD"></a>Lecture 3  GD</h1><h2 id="Different-gradient-descent"><a href="#Different-gradient-descent" class="headerlink" title="Different gradient descent."></a>Different gradient descent.</h2><p>Challenges: proper lr, same lr, saddle point</p>
<p>P9-10 Momentum</p>
<p>P11 NAG</p>
<p>P17 Adagrad</p>
<p>P21 Adadelta</p>
<p>P25 RMSprop</p>
<p>P26, 27 Adam</p>
<p>P34 Initialization</p>
<p>Batch Gradient descent: Accuracy </p>
<p>Stochastic gradient descent: efficiency</p>
<p>Mini-batch gradient descent: trade-off of accuracy and efficiency</p>
<p>Momentum is to update SGD, to accelerate and dampen oscillation</p>
<p>increase the momentum term in the same direction, reduce in change direction</p>
<p><strong>NAG</strong>: </p>
<p>The big jump in the standard momentum is too aggressive</p>
<p>So, in the Nesterov accelerated gradient use a previous gradient to big jump and a correction</p>
<p>![Untitled](<a href="https://zjsygqs398.github.io/blog/post_img/COMP">https://zjsygqs398.github.io/blog/post_img/COMP</a> 5329&#x2F;Untitled.png)</p>
<p>theta - r v_t-1 means that update according to the previous, which is the big jump depending on the previous accumulated gradient.</p>
<p>Then, calculate the gradient plus the contributed previously accumulated gradient to update the theta</p>
<p><strong>Adagrad:</strong></p>
<p>to achieve the different learning rate for features</p>
<p>i means dimension</p>
<p>t means iteration</p>
<p>suitable for sparse data</p>
<p>but the global learning rate needed</p>
<p><strong>Adadelta:</strong></p>
<p>![Untitled](<a href="https://zjsygqs398.github.io/blog/post_img/COMP">https://zjsygqs398.github.io/blog/post_img/COMP</a> 5329&#x2F;Untitled%201.png)</p>
<p>in order to deal with the problem of Adagrad(infinitesimally small for the denominator in the end), modify the gt equation as above. Contributed the last square sum of gt, and current gt.</p>
<p>H is the secondary gradient</p>
<p>with consistent units</p>
<h1 id="Lecture-4-Norm"><a href="#Lecture-4-Norm" class="headerlink" title="Lecture 4 Norm"></a>Lecture 4 Norm</h1><p>P10 weight decay</p>
<p>pls write the structure of the normalization</p>
<p>inverted, dropout-connect</p>
<p>BN, (reduce covariance)</p>
<p>R means regularisation functions, r on page 6 is the upper boundary for parameter complex</p>
<p>Page 24 in slide, bottom 2 are independent methods</p>
<p>the group right top corner is the normal training process(i.e. only train on the training set)</p>
<p>drop out</p>
<p>scale down</p>
<p>in the training process, each unit has with p possibility </p>
<p>p present</p>
<p>1-p disappear</p>
<p>in the test process, they always present</p>
<p>p is times the w  in layer </p>
<p>inverted</p>
<p>see slide</p>
<p>M in slide 36 is the matrix of binary (1 or 0)</p>
<p>adding the extra linear function after the normalization in each layer’s output. gamma means the scale parameter, beta means the shift parameter</p>
<p>batch norm: norm applies in all examples in this batch in each channel</p>
<p>layer norm: norm applies in each data sample in this batch</p>
<p>instance norm: norm applies in an example in this batch in a channel</p>
<p>group norm: norms apply in some channels in an example (split an example into multiple parts) in a data batch</p>
<h1 id="Lecture-5-CNN"><a href="#Lecture-5-CNN" class="headerlink" title="Lecture 5  CNN"></a>Lecture 5  CNN</h1><p>P46 different sorts of pooling</p>
<p>Spectral pooling?</p>
<p>un-pooling uses the (max) location information to reverse the process, keeping the 0 at information lacking place</p>
<p>more previous layer, the information is more simple. (line, color block)</p>
<p>more higher layer, the more meaningful information included</p>
<p>transposed convolution is also called the deconvolution method, enlarging the feature map until keeps the same as input size</p>
<p>in the process of deconvolution, the output will larger, it will happen to overlap in the process, summation the overlap region. Also, it has the stride, crop (like the padding, but crop the data in the output) Output size &#x3D; (N-1) S + K - 2C</p>
<p>In pytorch, using the sequential in init rather than init function like tut note.<br>the kernel in CNN is similar to the digital, tut 6 notebook</p>
<p>its like the mouse in 5318, get a bigger score of the kernel output</p>
<h1 id="Lecture-7"><a href="#Lecture-7" class="headerlink" title="Lecture 7"></a>Lecture 7</h1><p>dead relu means that the zero output of the relu activation function</p>
<p>Data augmentation is rotate, distortion, color changing etc. methods to manipulate the data.</p>
<p>In local response normalization, a is the center of the region</p>
<p>overlapping pooling, polling stride less than pooling kernel size</p>
<p>smaller filter means deeper, more non-linearity, fewer parameters.</p>
<p>p is changing to ensure the size of the output is same on the <strong>googlenet</strong></p>
<p>p28, right is the google net structure, and left is the normal way. It helps to decrease the depth of the feature map in the right method.</p>
<p>p37 is used to avoid the overconfident of the model, with the label smoothing</p>
<p>1 by 1 conv layer is used to exchange the information among channels ????</p>
<p>Shuffle net is another way to exchange information</p>
<p><strong>Extension</strong></p>
<p>Ghost filter?</p>
<p>Lego filter?</p>
<p>adder filter?</p>
<h1 id="Lecture-8"><a href="#Lecture-8" class="headerlink" title="Lecture 8"></a>Lecture 8</h1><h1 id="Lecture-9"><a href="#Lecture-9" class="headerlink" title="Lecture 9"></a>Lecture 9</h1><p>The masked LM in BERT in used to predict the masked word by linear regression</p>
<p>Classification in BERT is used in the binary classifier to identify the 1 or 2 sentences</p>
<p>P49 the place of dot product is, output of the classifier and the representation of token, multiple numbers is because the multiple-classifier of the model, then using the result passing into the softmax to predict the final prediction</p>
<p>the gates in code split the matrix into multiple chunks and pass into the gates.</p>
<h1 id="Lecture-10"><a href="#Lecture-10" class="headerlink" title="Lecture 10"></a>Lecture 10</h1><p>P26: if the degree of the J is large, the information from J to I node is very small. Because the denominator is Djj Dii.</p>
<p>the D^-0.5 A D^-0.5(connection) H(value of each neighbour, near features) W(parameter)</p>
<p>Laplacian means to reduce the shape of the matrix, in the spectral matrix, the shape is quality large</p>
<p>Each row and each column in matrix of Laplacian are zero</p>
<p>![Untitled](<a href="https://zjsygqs398.github.io/blog/post_img/COMP">https://zjsygqs398.github.io/blog/post_img/COMP</a> 5329&#x2F;Untitled%202.png)</p>
<p>the eigen matrix are used to reshape the original matrix, which similar to the PCA, focusing on the important part of the matrix</p>
<h1 id="Lecture-11"><a href="#Lecture-11" class="headerlink" title="Lecture 11"></a>Lecture 11</h1><p>the detected size of the boundary box many different. So, the resizing of the image is necessary for CNN. P16.  </p>
<p>The proposal may overlap, it makes the duplication of calculation. P17</p>
<p>Fast R-CNN: instead of using the image level input like P16, the fast way is using the features in CNN as input to the bounding box task. P20</p>
<p>But the problem of different sizes still exists, fast R-CNN uses the max pooling way to ensure the size of the output.</p>
<p>Faster R-CNN: spp-net, improve at the extra pooling layer then fast R-CNN. In this model, the classification, detection, boundary box, etc tasks are all done by different CNNs.</p>
<p>In other words, the nature of the faster here is replacing the original image object detection by the feature object detection. It reduces the computational resources.</p>
<p>Mask R-CNN: deter the pixel in the image along to what label. </p>
<p>RoIAlign is more accurate in the pooling layer, directly dividing the result evenly without considering the number, and using the distance of each point to decide the number of the results after the max pooling.</p>
<p>excepted predict the xyhw, the dx,dy,dh,dw (difference of the xyhw) are also included in the prediction. Jargon called an anchor.</p>
<h1 id="Lecture-12"><a href="#Lecture-12" class="headerlink" title="Lecture 12"></a>Lecture 12</h1><p>high value in Dx, maximize the (1-D(G(z)))</p>
<h1 id="Lecture-13"><a href="#Lecture-13" class="headerlink" title="Lecture 13"></a>Lecture 13</h1><p>diffusion</p>
<p>GAN: 输入为随机输入</p>
<p>f(theta) &#x3D; Y</p>
<p>Y 为图像</p>
<p>X 为随机输入</p>
<p>F（）为神经网络， 映射xy之间的转换关系，从正态分布到图像的分布（特殊的集合）</p>
<p>不用MSE的原因是 噪音和图像之前的关系没有有意义的关系，也就是梯度会没有， 并且只能生成看到过的图片， 不会生成没看见过的图片</p>
<p>生成式对抗模型：两个神经网络，生成式网络（生成假图）和分类器（检验真假）</p>
<p>生成式模型从分类起学会到如何生成真图</p>
<p>分类无法区分的时候，就训练完了</p>
<p>VAE: encoder the image, then decode it</p>
<p>decode之后和原始的 计算loss</p>
<p>从参化</p>
<p>diffusion:  去燥模型，退热定律</p>
<p>X → XT.noisy(高斯)</p>
<p>X → X+Theta → XT.noisy(高斯)</p>
<p>X → … → X+Theta * n → XT.noisy(高斯)</p>
<p>每次加一点点噪音， 直到纯噪音</p>
<p>每次加的高斯分布的噪音</p>
<p>loss 为检测图片中的噪音数量</p>
<p>反过去，就是按照噪音去预测图片</p>
<p>diffusion只有一个NN，其他模型为多个</p>
</div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/blog/tags/Pytorch/">Pytorch</a><a class="link-muted mr-2" rel="tag" href="/blog/tags/Machine-Learning/">Machine Learning</a><a class="link-muted mr-2" rel="tag" href="/blog/tags/Deep-Learning/">Deep Learning</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/blog/2024/06/16/COMP%205046/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">USYD - COMP 5046</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/blog/2024/06/16/DATA%205207/"><span class="level-item">USYD - DATA 5207</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/blog/my_img/avatar.png" alt="Quansui"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Quansui</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Sydney, Australia</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/blog/archives"><p class="title">21</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/blog/categories"><p class="title">3</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/blog/tags"><p class="title">25</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/zjsygqs398" target="_blank" rel="noopener">Follow</a></div></div></div><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/blog/categories/Internship/"><span class="level-start"><span class="level-item">Internship</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Pytorch-Introduction/"><span class="level-start"><span class="level-item">Pytorch Introduction</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/USYD-Lecture/"><span class="level-start"><span class="level-item">USYD - Lecture</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-06-16T00:12:25.000Z">2024-06-16</time></p><p class="title"><a href="/blog/2024/06/16/COMP%205046/">USYD - COMP 5046</a></p><p class="categories"><a href="/blog/categories/USYD-Lecture/">USYD - Lecture</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-06-16T00:10:20.000Z">2024-06-16</time></p><p class="title"><a href="/blog/2024/06/16/COMP%205329/">USYD - COMP 5329</a></p><p class="categories"><a href="/blog/categories/USYD-Lecture/">USYD - Lecture</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-06-16T00:08:50.000Z">2024-06-16</time></p><p class="title"><a href="/blog/2024/06/16/DATA%205207/">USYD - DATA 5207</a></p><p class="categories"><a href="/blog/categories/USYD-Lecture/">USYD - Lecture</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-06-15T02:10:42.000Z">2024-06-15</time></p><p class="title"><a href="/blog/2024/06/15/INFO%205992/">USYD - INFO 5992</a></p><p class="categories"><a href="/blog/categories/USYD-Lecture/">USYD - Lecture</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-12-21T21:06:08.000Z">2023-12-22</time></p><p class="title"><a href="/blog/2023/12/22/Tutorial%205/">Pytorch Tutorial 5</a></p><p class="categories"><a href="/blog/categories/Pytorch-Introduction/">Pytorch Introduction</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/blog/archives/2024/"><span class="level-start"><span class="level-item">2024</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/blog/archives/2023/"><span class="level-start"><span class="level-item">2023</span></span><span class="level-end"><span class="level-item tag">15</span></span></a></li><li><a class="level is-mobile" href="/blog/archives/2022/"><span class="level-start"><span class="level-item">2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/blog/tags/Cache/"><span class="tag">Cache</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Data-Analyse/"><span class="tag">Data Analyse</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Deep-Learning/"><span class="tag">Deep Learning</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/GIT/"><span class="tag">GIT</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/IT-Innovation/"><span class="tag">IT Innovation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/JPA/"><span class="tag">JPA</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Java/"><span class="tag">Java</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Linear-Regression/"><span class="tag">Linear Regression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/MQ/"><span class="tag">MQ</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/NLP/"><span class="tag">NLP</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Natural-Language-Processing/"><span class="tag">Natural Language Processing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Python/"><span class="tag">Python</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Pytorch/"><span class="tag">Pytorch</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/R/"><span class="tag">R</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Redis/"><span class="tag">Redis</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Schedule/"><span class="tag">Schedule</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Security/"><span class="tag">Security</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Shiro/"><span class="tag">Shiro</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Social-Science/"><span class="tag">Social Science</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/SpringBoot/"><span class="tag">SpringBoot</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Startups/"><span class="tag">Startups</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Swagger/"><span class="tag">Swagger</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Thread/"><span class="tag">Thread</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Tools/"><span class="tag">Tools</span><span class="tag">2</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/blog/"><img src="/blog/my_img/logo.svg" alt="Hexo" height="28"></a><p class="is-size-7"><span>&copy; 2024 John Doe</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/zjsygqs398"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/blog/js/column.js"></script><script src="/blog/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/blog/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/blog/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/blog/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/blog/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>