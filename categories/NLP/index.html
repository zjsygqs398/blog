<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><title>Category: NLP - Hexo</title><link rel="manifest" href="/blog/manifest.json"><meta name="application-name" content="Quansui&#039;s Blog"><meta name="msapplication-TileImage" content="/my_img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Quansui&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Hexo"><meta property="og:url" content="https://zjsygqs398.github.io/blog"><meta property="og:site_name" content="Hexo"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://zjsygqs398.github.io/blog/img/og_image.png"><meta property="article:author" content="John Doe"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://zjsygqs398.github.io/blog/img/og_image.png"><meta property="fb:app_id"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://zjsygqs398.github.io/blog"},"headline":"Hexo","image":["https://zjsygqs398.github.io/blog/img/og_image.png"],"author":{"@type":"Person","name":"John Doe"},"publisher":{"@type":"Organization","name":"Hexo","logo":{"@type":"ImageObject","url":"https://zjsygqs398.github.io/my_img/logo.svg"}},"description":""}</script><link rel="icon" href="/blog/my_img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/blog/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/blog/"><img src="/blog/my_img/logo.svg" alt="Hexo" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/blog/">Home</a><a class="navbar-item" href="/blog/archives">Archives</a><a class="navbar-item" href="/blog/categories">Categories</a><a class="navbar-item" href="/blog/tags">Tags</a><a class="navbar-item" target="_blank" rel="noopener" href="https://github.com/zjsygqs398">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/zjsygqs398"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/blog/categories">Categories</a></li><li class="is-active"><a href="#" aria-current="page">NLP</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-08-06T11:04:00.000Z" title="8/6/2024, 9:04:00 PM">2024-08-06</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-08-06T11:03:11.144Z" title="8/6/2024, 9:03:11 PM">2024-08-06</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/NLP/">NLP</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2024/08/06/tBERT%20f11ade6e1adc4549b0771fe588bfa2ae/">tBERT</a></p><div class="content"><h1 id="tBERT"><a href="#tBERT" class="headerlink" title="tBERT"></a>tBERT</h1><h2 id="Original-address-of-paper"><a href="#Original-address-of-paper" class="headerlink" title="Original address of paper"></a>Original address of paper</h2><p><a target="_blank" rel="noopener" href="https://www.aclweb.org/anthology/2020.acl-main.630.pdf">https://www.aclweb.org/anthology/2020.acl-main.630.pdf</a></p>
<h2 id="Reason-the-idea-come-up"><a href="#Reason-the-idea-come-up" class="headerlink" title="Reason the idea come up"></a>Reason the idea come up</h2><p>The semantic similarity could provide additional signal and information to model. Also, the integration of the domain could effectively help the model get better performance. That is the reason tBERT come up.</p>
<h2 id="Main-idea-of-the-technology"><a href="#Main-idea-of-the-technology" class="headerlink" title="Main idea of the technology"></a>Main idea of the technology</h2><p>First create the place for CLS in BERT, representing the domain of the input.</p>
<p>The input of sentences put into the topic model and get the average value.</p>
<p>At the meantime, the input the sentences into the BERT</p>
<p>The paper not mention the detail design of loss, from my understanding, the CLS should be included in the loss, and no other digest article for the tBERT, may it should be confirmed for the further research.</p>
<h2 id="Thinking"><a href="#Thinking" class="headerlink" title="Thinking"></a>Thinking</h2><p>Fine tuning could also helps model to deal with the domain shift, but the experiment showed that the result of tBERT is better than the fine tuning, especially for the specific domain, e.g., medical, finance, etc.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-08-06T11:02:00.000Z" title="8/6/2024, 9:02:00 PM">2024-08-06</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-08-06T11:02:49.270Z" title="8/6/2024, 9:02:49 PM">2024-08-06</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/NLP/">NLP</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2024/08/06/UniLM%202ba8f4d529ad496a876f74325cffeb9d/">UniLM</a></p><div class="content"><h1 id="UniLM"><a href="#UniLM" class="headerlink" title="UniLM"></a>UniLM</h1><p>The term stands for Unified Language Model Pre-training for Natural Language Understanding and Generation</p>
<h2 id="Original-address-of-paper"><a href="#Original-address-of-paper" class="headerlink" title="Original address of paper"></a>Original address of paper</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1905.03197.pdf">https://arxiv.org/pdf/1905.03197.pdf</a></p>
<h2 id="Reason-the-idea-come-up"><a href="#Reason-the-idea-come-up" class="headerlink" title="Reason the idea come up"></a>Reason the idea come up</h2><p>The reason of the BERT not good for NLG is the MLM(mask language model), which is not the same with the target of generation task. Except the masked position, other words are seen for model. Therefore, to improve the NLG performance of BERT, one way to handle is get better NLG ability.</p>
<h2 id="Main-idea-of-the-technology"><a href="#Main-idea-of-the-technology" class="headerlink" title="Main idea of the technology"></a>Main idea of the technology</h2><p>The author thought that the natural way to control the model is the mode of information input in the model. E.g., the access to the context of the word token to be predicted. The paper proposed the 3 ways: bidirectional LM, unidirectional LM, sequence-to-sequence LM.</p>
<p>For the special token, SOS, EOS stand for start and end of sentence. They helps to improve the ability of NLU and NLG, it marks the end of sequence, and help model to learn the when to terminate the NLG task.</p>
<p>In the unidirectional LM, comprised by left to right and right to left LM. Taking the example of left to right, the x1, x2, [mask], x4. The representation of mask only x1 and x2</p>
<p>In the bidirectional LM contains both directions context to represent the masked token.</p>
<p>In the sequence to sequence LM, the input has segment, that is the source and the target segments. SOS, t1 ,t2, EOS t3, t4, t5, EOS. t2 could access the first 4 tokens, t5, could only get the first 6 tokens. (including the SOS and EOS)</p>
<p>Also, the paper mentioned that the 1&#x2F;3 time use the bidirectional LM objective, 1&#x2F;3 time used the seq2seqLM objective, and the left to right and right to left are 1&#x2F;6 of the time be set as the objective. By this strategy, the final objective of UniLM could be achieve, i.e., generative performance.</p>
<p>In more details, the initialisation of UniLM used BERT-large, and the strategy of mask is same with BERT, but 20% of chance will be bigram or trigram, to improve the predictive performance.</p>
<h2 id="Terms-appear-in-the-paper"><a href="#Terms-appear-in-the-paper" class="headerlink" title="Terms appear in the paper"></a>Terms appear in the paper</h2><p>There are two new terms of NLP:</p>
<p>NLU: Natural language understanding</p>
<p>NLG: Natural language Generation</p>
<p>The concepts are not brand new, but the abbreviations are good for extension</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-08-04T11:07:00.000Z" title="8/4/2024, 9:07:00 PM">2024-08-04</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-08-06T11:01:35.130Z" title="8/6/2024, 9:01:35 PM">2024-08-06</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/NLP/">NLP</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2024/08/04/RoBERTa%20&amp;%20BERT%20space%20bias/">RoBERTa &amp; BERT embedding space bias</a></p><div class="content"><h1 id="RoBERTa"><a href="#RoBERTa" class="headerlink" title="RoBERTa"></a>RoBERTa</h1><h2 id="Original-address-of-paper"><a href="#Original-address-of-paper" class="headerlink" title="Original address of paper"></a>Original address of paper</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1907.11692">https://arxiv.org/abs/1907.11692</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/149249619">https://zhuanlan.zhihu.com/p/149249619</a></p>
<p>根据知乎的文章，总结出以下几点特征：</p>
<ol>
<li>用更长的时间和更大的BS，训练更多的数据</li>
<li>去掉BERT中的NSP（next sentence prediction)</li>
<li>在更长的句子上训练</li>
<li>根据训练数据动态的改变mask模式</li>
</ol>
<h2 id="Reason-the-idea-come-up"><a href="#Reason-the-idea-come-up" class="headerlink" title="Reason the idea come up"></a>Reason the idea come up</h2><p>从BERT的角度来说，masking是在预处理的时候执行Masked Language Model. In random masking, BERT will select the 15% word in each sentence, and with 80% chance replaced by MASK, 10% chance replaced by other random word, 10% keep the same. Recall the COMP 5046 unit, assignment, there are same. Thus, the model target is to predict the place be marked.</p>
<p>Also, BERT could predict the adjacent position relationship between sentences, that is called Next Sentence Prediction(NSP)</p>
<h3 id="Difference"><a href="#Difference" class="headerlink" title="Difference:"></a>Difference:</h3><p>其中比较大的变化是对于mask的处理方式不同</p>
<p>BERT是在pre-precessing的时候对于masking进行处理，这样一来masking是静态的，也就是对于每一个epoch来说masking的结果是一致的。为了避免这样的情况，会对数据进行复制，作出不同的静态masking进行训练。但还是会出现重复的现象。</p>
<p>而RoBERTa使用动态masking，在序列进入模型之前才处理masking的任务。</p>
<p>RoBERT remove the task of NSP, it consider this task is not improve the performance in expected.</p>
<h2 id="思考：BERT输出信息的继续挖掘以达到更好的语义理解"><a href="#思考：BERT输出信息的继续挖掘以达到更好的语义理解" class="headerlink" title="思考：BERT输出信息的继续挖掘以达到更好的语义理解"></a>思考：BERT输出信息的继续挖掘以达到更好的语义理解</h2><p>On the sentence embeddings from pre-trained language models</p>
<p>this is the address for original paper</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2011.05864.pdf">https://arxiv.org/pdf/2011.05864.pdf</a></p>
<p>The problems proposed in the paper mainly discuss the:</p>
<ol>
<li>why low performance of text matching in BERT for unsupervised task. Is that lacking of the sentiment information or wrong way to digging these information?</li>
<li>If lacking of digging, how to utilize them.</li>
</ol>
<p>论文给出的结论是我们对于BERT的输出信息利用不够强</p>
<p>论文中提出了一个方式，去映射BERT sentence embedding, called BERT-flow.</p>
<p>其启发是来自于1. 高频的偏差，对于高频词语的space来说，可能会导致向量的偏移，这会导致对于向量之间关系不按照我们所想的方式进行发展。2.sparse word会在BERT空间中比较稀疏（低频词），以及产生一些洞。这些洞会导致语义理解的难度</p>
<p>基于这两个问题， paper提出了一个转换的方式：</p>
<p>BERT embedding space to standard Gaussian latent space</p>
<p>从而实现了基于流的标定来帮助建立更加正确的BERT词向量</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-08-02T13:21:50.000Z" title="8/2/2024, 11:21:50 PM">2024-08-02</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-08-02T13:21:15.317Z" title="8/2/2024, 11:21:15 PM">2024-08-02</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/NLP/">NLP</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2024/08/02/FastBERT/">FastBERT解读</a></p><div class="content"><h1 id="FastBERT"><a href="#FastBERT" class="headerlink" title="FastBERT"></a>FastBERT</h1><p><a target="_blank" rel="noopener" href="https://aclanthology.org/2020.acl-main.537/">https://aclanthology.org/2020.acl-main.537/</a></p>
<p>首先给出模型的提出原文地址</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/127869267">https://zhuanlan.zhihu.com/p/127869267</a></p>
<p>对于FastBERT的定义和解读，知乎这篇文章讲的十分优秀，可以仔细阅读</p>
<p>简单来理解，单纯使用BERT对样本进行预测工作需要经过完整的网络，FastBERT的主要目的是为了让降低计算。</p>
<ol>
<li>从蒸馏的角度来说是创建一个全新的学生网络去拟合本地的大BERT模型从而实现对模型的简化。</li>
<li>但是FastBERT的思想是对于模型进行改造。聚焦于一些简单的样本，也就是不需要通过整个网络就可以进行分类的样本进行优化。解决方案为：给每一层后接上分类器。这样主要的结构就出现了。</li>
</ol>
<p>作者称之BERT模型为主干，Backbone</p>
<p>每一个分类器为分支，Branch</p>
<p>这一过程作者称之为自蒸馏，在pre-train &amp; fine-tuning 的时候只更新主干，tuning结束之后freeze主干用branch获得蒸馏结果</p>
<p>总结，fastBERT主要思想是提前输出简单样本，来减少模型的计算压力，从而提高推理速度。优点：不需要改变主要的BERT结构（相比于Distill-BERT）</p>
<p>思考：如何去决定当前批数据经过的层数，如何去定义？</p>
<p>在原文中提到，不确定性高于speed的将会被传入下一层，低于speed的就停止传递</p>
<p>其中，对于不确定性的定义是当前批的数据的煽值（正则化），其中包括一些假设：低不确定性对应高精度。这样子理解，speed在原文中的意义就是threshold，去定义以及划分不确定性的高低。越高的speed，越高的层收到的样本就越少，也就意味着门槛变低，更多的样本被直接输出，从而提高了模型的计算速度。</p>
<p><del>至于模型的loss计算，GPT给出的解释为分类loss和层选择的loss（CE, MSE[选择的层和全部的层]）但是觉得与原文有些出入，还需要进一步研究。（对于损失函数的观点未经证实）</del></p>
<p>本文记录了阅读FastBERT paper的笔记以及对于前辈的知识总结进行归纳。大致了解了FastBERT的思路和实现原理，但对于原文中的KL散度分布还有些困惑，需要进一步研究。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-08-01T06:48:00.000Z" title="8/1/2024, 4:48:00 PM">2024-08-01</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-08-06T11:00:59.632Z" title="8/6/2024, 9:00:59 PM">2024-08-06</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/NLP/">NLP</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2024/08/01/Knowledge%20Distillation/">Knowledge Distillation</a></p><div class="content"><h1 id="Knowledge-Distillation"><a href="#Knowledge-Distillation" class="headerlink" title="Knowledge Distillation"></a>Knowledge Distillation</h1><h2 id="Original-address-of-paper"><a href="#Original-address-of-paper" class="headerlink" title="Original address of paper"></a>Original address of paper</h2><p> <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/75031938">https://zhuanlan.zhihu.com/p/75031938</a></p>
<p>The main idea of the knowledge distillation is “forget the models in the ensemble and the way they are parameterised and focus on the function”</p>
<h2 id="Main-idea-of-the-technology"><a href="#Main-idea-of-the-technology" class="headerlink" title="Main idea of the technology"></a>Main idea of the technology</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1503.02531">https://arxiv.org/abs/1503.02531</a></p>
<p>在Hinton原文中指出训练时就如同毛毛虫吃树叶来积攒能量，在使用神经网络的时候既做了吃树叶又繁殖的任务，导致效率低下。因此对于这样的问题，希望将一个复杂的模型转变到简单的模型，也就是知识蒸馏做的事情。</p>
<p>蒸馏的过程就是将训练的大模型的内容总结到小模型中，已达到模型复杂度和精度的平衡。在这里会有两个模型，较大的为老师模型，小的模型为学生模型，老师模型通过对于hard targert 进行训练，而学生模型通过老师模型的输出进行收敛。</p>
<p>To achieve training of the student network, the paper mentioned that adding a temperature to scale down the target before input into the softmax.  (i.e., exp(zi&#x2F;T) &#x2F; sum(exp(zi&#x2F;T))) The more larger T in formula, the more smooth distribution the result will be.</p>
<p>The loss function is a * soft + (1-a) * hard, i.e. the trade-off between them.</p>
<p>in general, the larger soft distribution will get better performance in test.</p>
<ol>
<li>训练大模型，也就是使用hard taget， 也就是不对标签做处理</li>
<li>通过老师模型计算soft target</li>
<li>训练小模型<ol>
<li>设置相同T计算结果与soft target 计算loss （from 2）</li>
<li>设置T为1，也就是hard target计算loss</li>
</ol>
</li>
<li>学生模型T设置为1作为最后的预测</li>
</ol>
<p><strong>Summary</strong></p>
<p>The reason knowledge distillation worked is the too sharp distribution of result makes loss function not effect the model, i.e., except the true label, outputs are too close to 0, making the model could not learning anymore. Thus, the introducing of the T to scaling down the result helps to get smooth distribution. In this way, get simper model and better convergence of model.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-06-21T03:22:32.000Z" title="6/21/2024, 1:22:32 PM">2024-06-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-06-21T03:27:20.683Z" title="6/21/2024, 1:27:20 PM">2024-06-21</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/NLP/">NLP</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2024/06/21/SpaCy/">SpaCy</a></p><div class="content"><h1 id="SpaCy"><a href="#SpaCy" class="headerlink" title="SpaCy"></a>SpaCy</h1><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><ol>
<li>It contains the pip installation in python</li>
<li>created a english based nlp object</li>
<li>input some text to create the object of document</li>
<li>operations demo that can get the text from token</li>
<li>span of the token in text</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. installation</span></span><br><span class="line">!pip install spacy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Create a blank English nlp object</span></span><br><span class="line">nlp = spacy.blank(<span class="string">&quot;en&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Created by processing a string of text with the nlp object</span></span><br><span class="line">doc = nlp(<span class="string">&quot;Hello world!&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Index into the Doc to get a single Token</span></span><br><span class="line">token = doc[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the token text via the .text attribute</span></span><br><span class="line"><span class="built_in">print</span>(token.text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterate over tokens in a Doc</span></span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> doc:</span><br><span class="line">    <span class="built_in">print</span>(token.text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. Create another document with text</span></span><br><span class="line">doc = nlp(<span class="string">&quot;Hello NLP class!&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># A slice from the Doc is a Span object</span></span><br><span class="line">span = doc[<span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the span text via the .text attribute</span></span><br><span class="line"><span class="built_in">print</span>(span.text)</span><br></pre></td></tr></table></figure>

<p>demonstrations of <code>is_alpha</code>, <code>is_punct</code>, <code>like_num</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">doc = nlp(<span class="string">&quot;It costs $5.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Index:   &quot;</span>, [token.i <span class="keyword">for</span> token <span class="keyword">in</span> doc])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Text:    &quot;</span>, [token.text <span class="keyword">for</span> token <span class="keyword">in</span> doc])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;is_alpha:&quot;</span>, [token.is_alpha <span class="keyword">for</span> token <span class="keyword">in</span> doc])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;is_punct:&quot;</span>, [token.is_punct <span class="keyword">for</span> token <span class="keyword">in</span> doc])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;like_num:&quot;</span>, [token.like_num <span class="keyword">for</span> token <span class="keyword">in</span> doc])</span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># Index:    [0, 1, 2, 3, 4]</span></span><br><span class="line"><span class="comment"># Text:     [&#x27;It&#x27;, &#x27;costs&#x27;, &#x27;$&#x27;, &#x27;5&#x27;, &#x27;.&#x27;]</span></span><br><span class="line"><span class="comment"># is_alpha: [True, True, False, False, False]</span></span><br><span class="line"><span class="comment"># is_punct: [False, False, False, False, True]</span></span><br><span class="line"><span class="comment"># like_num: [False, False, False, True, False]</span></span><br></pre></td></tr></table></figure>

<h2 id="Part-of-Speech"><a href="#Part-of-Speech" class="headerlink" title="Part of Speech"></a>Part of Speech</h2><p>Load the small english pipeline to achieve the pos</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">!python -m spacy download en_core_web_sm</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the small English pipeline</span></span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Process a text</span></span><br><span class="line">doc = nlp(<span class="string">&quot;She ate the pizza&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterate over the tokens</span></span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> doc:</span><br><span class="line">    <span class="comment"># Print the text and the predicted part-of-speech tag</span></span><br><span class="line">    <span class="built_in">print</span>(token.text, token.pos_, token.pos)</span><br><span class="line"></span><br><span class="line"><span class="comment"># result: </span></span><br><span class="line"><span class="comment"># She PRON 95</span></span><br><span class="line"><span class="comment"># ate VERB 100</span></span><br><span class="line"><span class="comment"># the DET 90</span></span><br><span class="line"><span class="comment"># pizza NOUN 92</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># also the token.head returns the syntactic head token</span></span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> doc:</span><br><span class="line">    <span class="built_in">print</span>(token.text, token.pos_, token.dep_, token.head)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># She PRON nsubj ate</span></span><br><span class="line"><span class="comment"># ate VERB ROOT ate</span></span><br><span class="line"><span class="comment"># the DET det pizza</span></span><br><span class="line"><span class="comment"># pizza NOUN dobj ate</span></span><br></pre></td></tr></table></figure>

<h2 id="Name-Entity-Recognition"><a href="#Name-Entity-Recognition" class="headerlink" title="Name Entity Recognition"></a>Name Entity Recognition</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Process a text</span></span><br><span class="line">doc = nlp(<span class="string">&quot;Apple is looking at buying U.K. startup for $1 billion&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterate over the predicted entities</span></span><br><span class="line"><span class="keyword">for</span> ent <span class="keyword">in</span> doc.ents:</span><br><span class="line">    <span class="comment"># Print the entity text and its label</span></span><br><span class="line">    <span class="built_in">print</span>(ent.text, ent.label_)</span><br></pre></td></tr></table></figure>

<h2 id="Matching"><a href="#Matching" class="headerlink" title="Matching"></a>Matching</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import the Matcher</span></span><br><span class="line"><span class="keyword">from</span> spacy.matcher <span class="keyword">import</span> Matcher</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load a pipeline and create the nlp object</span></span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the matcher with the shared vocab</span></span><br><span class="line">matcher = Matcher(nlp.vocab)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add the pattern to the matcher</span></span><br><span class="line">pattern = [&#123;<span class="string">&quot;TEXT&quot;</span>: <span class="string">&quot;iPhone&quot;</span>&#125;, &#123;<span class="string">&quot;TEXT&quot;</span>: <span class="string">&quot;X&quot;</span>&#125;]</span><br><span class="line">matcher.add(<span class="string">&quot;IPHONE_PATTERN&quot;</span>, [pattern])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Process some text</span></span><br><span class="line">doc = nlp(<span class="string">&quot;iPhone X news! Upcoming iPhone X release date leaked&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Call the matcher on the doc</span></span><br><span class="line">matches = matcher(doc)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterate over the matches</span></span><br><span class="line"><span class="keyword">for</span> pattern_id, start, end <span class="keyword">in</span> matches:</span><br><span class="line">    <span class="comment"># Get the matched span</span></span><br><span class="line">    matched_span = doc[start:end]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;&quot;&#123;&#125;&quot; - match for pattern &#123;&#125; in span (&#123;&#125;, &#123;&#125;)&#x27;</span>.<span class="built_in">format</span>(matched_span.text, pattern_id, start, end))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># &quot;iPhone X&quot; - match for pattern 9528407286733565721 in span (0, 2)</span></span><br><span class="line"><span class="comment"># &quot;iPhone X&quot; - match for pattern 9528407286733565721 in span (5, 7)</span></span><br></pre></td></tr></table></figure>

<p>in match pattern, LEMMA contains the multiple variations of the word</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">pattern = [</span><br><span class="line">    &#123;<span class="string">&quot;LEMMA&quot;</span>: <span class="string">&quot;love&quot;</span>, <span class="string">&quot;POS&quot;</span>: <span class="string">&quot;VERB&quot;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;POS&quot;</span>: <span class="string">&quot;NOUN&quot;</span>&#125;</span><br><span class="line">]</span><br><span class="line">matcher = Matcher(nlp.vocab)</span><br><span class="line">matcher.add(<span class="string">&quot;LOVE_PATTERN&quot;</span>, [pattern])</span><br><span class="line"></span><br><span class="line">doc = nlp(<span class="string">&quot;I loved vanilla but now I love chocolate more.&quot;</span>)</span><br><span class="line"></span><br><span class="line">matches = matcher(doc)</span><br><span class="line"><span class="keyword">for</span> pattern_id, start, end <span class="keyword">in</span> matches:</span><br><span class="line">    matched_span = doc[start:end]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;&quot;&#123;&#125;&quot; - match for pattern &#123;&#125; in span (&#123;&#125;, &#123;&#125;)&#x27;</span>.<span class="built_in">format</span>(matched_span.text, pattern_id, start, end))</span><br><span class="line"></span><br><span class="line"><span class="comment"># &quot;loved vanilla&quot; - match for pattern 4358456325055851256 in span (1, 3)</span></span><br><span class="line"><span class="comment"># &quot;love chocolate&quot; - match for pattern 4358456325055851256 in span (6, 8)</span></span><br></pre></td></tr></table></figure>

<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Operators and quantifiers let you define how often a token should be matched. They can be added using the &quot;OP&quot; key. &quot;OP&quot; can have one of four values:</span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> An &quot;!&quot; negates the token, so it&#x27;s matched 0 times.</span><br><span class="line"><span class="bullet">-</span> A &quot;?&quot; makes the token optional, and matches it 0 or 1 times.</span><br><span class="line"><span class="bullet">-</span> A &quot;+&quot; matches a token 1 or more times.</span><br><span class="line"><span class="bullet">-</span> And finally, an &quot;<span class="emphasis">*&quot; matches 0 or more times.</span></span><br></pre></td></tr></table></figure>

<h2 id="Word-Vector"><a href="#Word-Vector" class="headerlink" title="Word Vector"></a>Word Vector</h2><p>SpaCy contains the word vector in medium pipeline</p>
<p>It could output the word vectors and calculate the similarity of tokens </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load a larger pipeline with vectors</span></span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_md&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Look at one word vector</span></span><br><span class="line">doc = nlp(<span class="string">&quot;I love chocolate&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(doc[<span class="number">2</span>].vector)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compare two documents</span></span><br><span class="line">doc1 = nlp(<span class="string">&quot;I like fast food&quot;</span>)</span><br><span class="line">doc2 = nlp(<span class="string">&quot;I like pizza&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Comparing sentences:&quot;</span>, doc1.similarity(doc2))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compare two tokens</span></span><br><span class="line">doc = nlp(<span class="string">&quot;I like pizza and pasta&quot;</span>)</span><br><span class="line">token1 = doc[<span class="number">2</span>]</span><br><span class="line">token2 = doc[<span class="number">4</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Comparing &#x27;pizza&#x27; and &#x27;paste&#x27;:&quot;</span>, token1.similarity(token2))</span><br></pre></td></tr></table></figure>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li><a target="_blank" rel="noopener" href="https://course.spacy.io/en/">https://course.spacy.io/en/</a></li>
<li><a target="_blank" rel="noopener" href="https://spacy.io/">https://spacy.io/</a></li>
</ol>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/blog/my_img/avatar.png" alt="Quansui"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Quansui</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Sydney, Australia</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/blog/archives"><p class="title">27</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/blog/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/blog/tags"><p class="title">27</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/zjsygqs398" target="_blank" rel="noopener">Follow</a></div></div></div><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/blog/categories/Internship/"><span class="level-start"><span class="level-item">Internship</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Pytorch-Introduction/"><span class="level-start"><span class="level-item">Pytorch Introduction</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/USYD-Lecture/"><span class="level-start"><span class="level-item">USYD - Lecture</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-08-06T11:04:00.000Z">2024-08-06</time></p><p class="title"><a href="/blog/2024/08/06/tBERT%20f11ade6e1adc4549b0771fe588bfa2ae/">tBERT</a></p><p class="categories"><a href="/blog/categories/NLP/">NLP</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-08-06T11:02:00.000Z">2024-08-06</time></p><p class="title"><a href="/blog/2024/08/06/UniLM%202ba8f4d529ad496a876f74325cffeb9d/">UniLM</a></p><p class="categories"><a href="/blog/categories/NLP/">NLP</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-08-04T11:07:00.000Z">2024-08-04</time></p><p class="title"><a href="/blog/2024/08/04/RoBERTa%20&amp;%20BERT%20space%20bias/">RoBERTa &amp; BERT embedding space bias</a></p><p class="categories"><a href="/blog/categories/NLP/">NLP</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-08-02T13:21:50.000Z">2024-08-02</time></p><p class="title"><a href="/blog/2024/08/02/FastBERT/">FastBERT解读</a></p><p class="categories"><a href="/blog/categories/NLP/">NLP</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-08-01T06:48:00.000Z">2024-08-01</time></p><p class="title"><a href="/blog/2024/08/01/Knowledge%20Distillation/">Knowledge Distillation</a></p><p class="categories"><a href="/blog/categories/NLP/">NLP</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/blog/archives/2024/"><span class="level-start"><span class="level-item">2024</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/blog/archives/2023/"><span class="level-start"><span class="level-item">2023</span></span><span class="level-end"><span class="level-item tag">15</span></span></a></li><li><a class="level is-mobile" href="/blog/archives/2022/"><span class="level-start"><span class="level-item">2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/blog/tags/BERT/"><span class="tag">BERT</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Cache/"><span class="tag">Cache</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Data-Analyse/"><span class="tag">Data Analyse</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Deep-Learning/"><span class="tag">Deep Learning</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/GIT/"><span class="tag">GIT</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/IT-Innovation/"><span class="tag">IT Innovation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/JPA/"><span class="tag">JPA</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Java/"><span class="tag">Java</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Linear-Regression/"><span class="tag">Linear Regression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/MQ/"><span class="tag">MQ</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/NLP/"><span class="tag">NLP</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Natural-Language-Processing/"><span class="tag">Natural Language Processing</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Python/"><span class="tag">Python</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Pytorch/"><span class="tag">Pytorch</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/R/"><span class="tag">R</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Redis/"><span class="tag">Redis</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Schedule/"><span class="tag">Schedule</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Security/"><span class="tag">Security</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Shiro/"><span class="tag">Shiro</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Social-Science/"><span class="tag">Social Science</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/SpaCy/"><span class="tag">SpaCy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/SpringBoot/"><span class="tag">SpringBoot</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Startups/"><span class="tag">Startups</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Swagger/"><span class="tag">Swagger</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Thread/"><span class="tag">Thread</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Tools/"><span class="tag">Tools</span><span class="tag">2</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/blog/"><img src="/blog/my_img/logo.svg" alt="Hexo" height="28"></a><p class="is-size-7"><span>&copy; 2024 John Doe</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/zjsygqs398"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/blog/js/column.js"></script><script src="/blog/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/blog/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/blog/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/blog/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/blog/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>