<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><title>Tag: Machine Learning - Hexo</title><link rel="manifest" href="/blog/manifest.json"><meta name="application-name" content="Quansui&#039;s Blog"><meta name="msapplication-TileImage" content="/my_img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Quansui&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Hexo"><meta property="og:url" content="https://zjsygqs398.github.io/blog"><meta property="og:site_name" content="Hexo"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://zjsygqs398.github.io/blog/img/og_image.png"><meta property="article:author" content="John Doe"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://zjsygqs398.github.io/blog/img/og_image.png"><meta property="fb:app_id"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://zjsygqs398.github.io/blog"},"headline":"Hexo","image":["https://zjsygqs398.github.io/blog/img/og_image.png"],"author":{"@type":"Person","name":"John Doe"},"publisher":{"@type":"Organization","name":"Hexo","logo":{"@type":"ImageObject","url":"https://zjsygqs398.github.io/my_img/logo.svg"}},"description":""}</script><link rel="icon" href="/blog/my_img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/blog/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/blog/"><img src="/blog/my_img/logo.svg" alt="Hexo" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/blog/">Home</a><a class="navbar-item" href="/blog/archives">Archives</a><a class="navbar-item" href="/blog/categories">Categories</a><a class="navbar-item" href="/blog/tags">Tags</a><a class="navbar-item" target="_blank" rel="noopener" href="https://github.com/zjsygqs398">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/zjsygqs398"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/blog/tags">Tags</a></li><li class="is-active"><a href="#" aria-current="page">Machine Learning</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-06-16T00:12:25.000Z" title="6/16/2024, 10:12:25 AM">2024-06-16</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-06-20T02:24:28.469Z" title="6/20/2024, 12:24:28 PM">2024-06-20</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/USYD-Lecture/">USYD - Lecture</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2024/06/16/COMP%205046/">USYD - COMP 5046</a></p><div class="content"><h1 id="Lecture-1"><a href="#Lecture-1" class="headerlink" title="Lecture 1"></a>Lecture 1</h1><p>N-Gram LMs</p>
<p>The 4 ways to deal with the unseen word sequence</p>
<p>smoothing:</p>
<p>![Untitled](post_img&#x2F;COMP 5046&#x2F;Untitled.png)</p>
<p>discounting:</p>
<p>![Untitled](.&#x2F;post_img&#x2F;COMP 5046&#x2F;Untitled%201.png)</p>
<p>Interpolation: using the multiple N-Gram probability </p>
<p>![Untitled](.&#x2F;post_img&#x2F;COMP 5046&#x2F;Untitled%202.png)</p>
<p>Kneser-Ney smoothing:</p>
<h1 id="Lecture2"><a href="#Lecture2" class="headerlink" title="Lecture2"></a>Lecture2</h1><h1 id="Lecture3"><a href="#Lecture3" class="headerlink" title="Lecture3"></a>Lecture3</h1><h1 id="Lecture4"><a href="#Lecture4" class="headerlink" title="Lecture4"></a>Lecture4</h1><p>Note the formula macro and micro F1 score — ass2</p>
<p>Not the find_best_socre initial number should be negative — ass2</p>
<p>directly using average will make some features vanish (+ and -), also the order is meaningless</p>
<p>in slide, NER page 66</p>
<p>labels number * length * length means:</p>
<p>each token in sequence is possible for start or end, that is length * length</p>
<p>then, each span can be predicted as each label</p>
<p>so, the output should be (labels * length * length)</p>
<p>![Untitled](.&#x2F;post_img&#x2F;COMP 5046&#x2F;Untitled%203.png)</p>
<p>![Untitled](.&#x2F;post_img&#x2F;COMP 5046&#x2F;Untitled%204.png)</p>
<h1 id="Lecture-5"><a href="#Lecture-5" class="headerlink" title="Lecture 5"></a>Lecture 5</h1><p>beam search</p>
<p>in nature, greedy search is to find the local optimal solution in each step</p>
<h1 id="Lecture-6"><a href="#Lecture-6" class="headerlink" title="Lecture 6"></a>Lecture 6</h1><p>the label in current word from LSTM purely, B I O possibility(sum as 1)</p>
<p>score &#x3D; current label p * best (previous p * transfer p)</p>
<p>it from </p>
<p>score &#x3D; current label p * best (i.e. DP * transfer model)</p>
<p>DP refers to the optimal route in choice</p>
<p>transfer model refers to the Markov model</p>
<p>co reference</p>
<p>dependency parsing</p>
<h1 id="Lecture-7"><a href="#Lecture-7" class="headerlink" title="Lecture 7"></a>Lecture 7</h1><h1 id="Lecture-8"><a href="#Lecture-8" class="headerlink" title="Lecture 8"></a>Lecture 8</h1><p>disadvantages of self-attention</p>
<p><strong>problem:</strong></p>
<p>non-linear lacking</p>
<p>order lacking</p>
<p><strong>resolve:</strong></p>
<p>feedforward layer</p>
<p>additional position vector</p>
<p>issue: the length of the position vector is fixed, not flexible enough<br>the mask is for ignoring the future word in the training process, it lets the model know what is known in this step, and what is the prediction in this step. In math, put the infinite negative value for future words and dot the product in the current word<br>the multiple layers of self-attention are used to build better representation of the word</p>
<p>query: curve line </p>
<p>key: embedding word</p>
<p>value: passing into the weighted value</p>
<h1 id="Lecture-10"><a href="#Lecture-10" class="headerlink" title="Lecture 10"></a>Lecture 10</h1><p>data can from </p>
<p>fineweb</p>
<p>common craw</p>
<h1 id="Lecture-11"><a href="#Lecture-11" class="headerlink" title="Lecture 11"></a>Lecture 11</h1><h1 id="Review"><a href="#Review" class="headerlink" title="Review"></a>Review</h1><p>The content below is the missing slide week.</p>
<p>parsing, 句法分析, identity the grammar structure by sub-phrases</p>
<p>The span in the sentence called the phrase</p>
<p>treebanks</p>
<p>Method: Dependency Parsing &#x2F; Grammar</p>
<p>compute every possible dependency to find the best score</p>
<p>Or, choose a way to get the best overall score</p>
<p>co-reference</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-06-16T00:10:20.000Z" title="6/16/2024, 10:10:20 AM">2024-06-16</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-06-20T01:33:55.700Z" title="6/20/2024, 11:33:55 AM">2024-06-20</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/USYD-Lecture/">USYD - Lecture</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2024/06/16/COMP%205329/">USYD - COMP 5329</a></p><div class="content"><h1 id="COMP-5329"><a href="#COMP-5329" class="headerlink" title="COMP 5329"></a>COMP 5329</h1><h1 id="Lecture-2"><a href="#Lecture-2" class="headerlink" title="Lecture 2"></a>Lecture 2</h1><p>P9 XOR</p>
<p>P31 Activation</p>
<p>P36 CE</p>
<p>P37 KL, Entropy</p>
<p>P38 SoftMax</p>
<p>p36, p37  two attributes cross-entropy loss issue?<br>the disadvantage of batch gradient descent is when N is too large, the computation is very expensive, but the one example SGD only uses the single example. It may not be the best, due to the random.</p>
<p>mini-batches SGD: divide into mini-batches, in each epoch calculate the gradient</p>
<p>extension content:</p>
<p>sensitivity? make the back propagation similar to the feedforward</p>
<h1 id="Lecture-3-GD"><a href="#Lecture-3-GD" class="headerlink" title="Lecture 3  GD"></a>Lecture 3  GD</h1><h2 id="Different-gradient-descent"><a href="#Different-gradient-descent" class="headerlink" title="Different gradient descent."></a>Different gradient descent.</h2><p>Challenges: proper lr, same lr, saddle point</p>
<p>P9-10 Momentum</p>
<p>P11 NAG</p>
<p>P17 Adagrad</p>
<p>P21 Adadelta</p>
<p>P25 RMSprop</p>
<p>P26, 27 Adam</p>
<p>P34 Initialization</p>
<p>Batch Gradient descent: Accuracy </p>
<p>Stochastic gradient descent: efficiency</p>
<p>Mini-batch gradient descent: trade-off of accuracy and efficiency</p>
<p>Momentum is to update SGD, to accelerate and dampen oscillation</p>
<p>increase the momentum term in the same direction, reduce in change direction</p>
<p><strong>NAG</strong>: </p>
<p>The big jump in the standard momentum is too aggressive</p>
<p>So, in the Nesterov accelerated gradient use a previous gradient to big jump and a correction</p>
<p>![Untitled](.&#x2F;post_img&#x2F;COMP 5329&#x2F;Untitled.png)</p>
<p>theta - r v_t-1 means that update according to the previous, which is the big jump depending on the previous accumulated gradient.</p>
<p>Then, calculate the gradient plus the contributed previously accumulated gradient to update the theta</p>
<p><strong>Adagrad:</strong></p>
<p>to achieve the different learning rate for features</p>
<p>i means dimension</p>
<p>t means iteration</p>
<p>suitable for sparse data</p>
<p>but the global learning rate needed</p>
<p><strong>Adadelta:</strong></p>
<p>![Untitled](.&#x2F;post_img&#x2F;COMP 5329&#x2F;Untitled%201.png)</p>
<p>in order to deal with the problem of Adagrad(infinitesimally small for the denominator in the end), modify the gt equation as above. Contributed the last square sum of gt, and current gt.</p>
<p>H is the secondary gradient</p>
<p>with consistent units</p>
<h1 id="Lecture-4-Norm"><a href="#Lecture-4-Norm" class="headerlink" title="Lecture 4 Norm"></a>Lecture 4 Norm</h1><p>P10 weight decay</p>
<p>pls write the structure of the normalization</p>
<p>inverted, dropout-connect</p>
<p>BN, (reduce covariance)</p>
<p>R means regularisation functions, r on page 6 is the upper boundary for parameter complex</p>
<p>Page 24 in slide, bottom 2 are independent methods</p>
<p>the group right top corner is the normal training process(i.e. only train on the training set)</p>
<p>drop out</p>
<p>scale down</p>
<p>in the training process, each unit has with p possibility </p>
<p>p present</p>
<p>1-p disappear</p>
<p>in the test process, they always present</p>
<p>p is times the w  in layer </p>
<p>inverted</p>
<p>see slide</p>
<p>M in slide 36 is the matrix of binary (1 or 0)</p>
<p>adding the extra linear function after the normalization in each layer’s output. gamma means the scale parameter, beta means the shift parameter</p>
<p>batch norm: norm applies in all examples in this batch in each channel</p>
<p>layer norm: norm applies in each data sample in this batch</p>
<p>instance norm: norm applies in an example in this batch in a channel</p>
<p>group norm: norms apply in some channels in an example (split an example into multiple parts) in a data batch</p>
<h1 id="Lecture-5-CNN"><a href="#Lecture-5-CNN" class="headerlink" title="Lecture 5  CNN"></a>Lecture 5  CNN</h1><p>P46 different sorts of pooling</p>
<p>Spectral pooling?</p>
<p>un-pooling uses the (max) location information to reverse the process, keeping the 0 at information lacking place</p>
<p>more previous layer, the information is more simple. (line, color block)</p>
<p>more higher layer, the more meaningful information included</p>
<p>transposed convolution is also called the deconvolution method, enlarging the feature map until keeps the same as input size</p>
<p>in the process of deconvolution, the output will larger, it will happen to overlap in the process, summation the overlap region. Also, it has the stride, crop (like the padding, but crop the data in the output) Output size &#x3D; (N-1) S + K - 2C</p>
<p>In pytorch, using the sequential in init rather than init function like tut note.<br>the kernel in CNN is similar to the digital, tut 6 notebook</p>
<p>its like the mouse in 5318, get a bigger score of the kernel output</p>
<h1 id="Lecture-7"><a href="#Lecture-7" class="headerlink" title="Lecture 7"></a>Lecture 7</h1><p>dead relu means that the zero output of the relu activation function</p>
<p>Data augmentation is rotate, distortion, color changing etc. methods to manipulate the data.</p>
<p>In local response normalization, a is the center of the region</p>
<p>overlapping pooling, polling stride less than pooling kernel size</p>
<p>smaller filter means deeper, more non-linearity, fewer parameters.</p>
<p>p is changing to ensure the size of the output is same on the <strong>googlenet</strong></p>
<p>p28, right is the google net structure, and left is the normal way. It helps to decrease the depth of the feature map in the right method.</p>
<p>p37 is used to avoid the overconfident of the model, with the label smoothing</p>
<p>1 by 1 conv layer is used to exchange the information among channels ????</p>
<p>Shuffle net is another way to exchange information</p>
<p><strong>Extension</strong></p>
<p>Ghost filter?</p>
<p>Lego filter?</p>
<p>adder filter?</p>
<h1 id="Lecture-8"><a href="#Lecture-8" class="headerlink" title="Lecture 8"></a>Lecture 8</h1><h1 id="Lecture-9"><a href="#Lecture-9" class="headerlink" title="Lecture 9"></a>Lecture 9</h1><p>The masked LM in BERT in used to predict the masked word by linear regression</p>
<p>Classification in BERT is used in the binary classifier to identify the 1 or 2 sentences</p>
<p>P49 the place of dot product is, output of the classifier and the representation of token, multiple numbers is because the multiple-classifier of the model, then using the result passing into the softmax to predict the final prediction</p>
<p>the gates in code split the matrix into multiple chunks and pass into the gates.</p>
<h1 id="Lecture-10"><a href="#Lecture-10" class="headerlink" title="Lecture 10"></a>Lecture 10</h1><p>P26: if the degree of the J is large, the information from J to I node is very small. Because the denominator is Djj Dii.</p>
<p>the D^-0.5 A D^-0.5(connection) H(value of each neighbour, near features) W(parameter)</p>
<p>Laplacian means to reduce the shape of the matrix, in the spectral matrix, the shape is quality large</p>
<p>Each row and each column in matrix of Laplacian are zero</p>
<p>![Untitled](.&#x2F;post_img&#x2F;COMP 5329&#x2F;Untitled%202.png)</p>
<p>the eigen matrix are used to reshape the original matrix, which similar to the PCA, focusing on the important part of the matrix</p>
<h1 id="Lecture-11"><a href="#Lecture-11" class="headerlink" title="Lecture 11"></a>Lecture 11</h1><p>the detected size of the boundary box many different. So, the resizing of the image is necessary for CNN. P16.  </p>
<p>The proposal may overlap, it makes the duplication of calculation. P17</p>
<p>Fast R-CNN: instead of using the image level input like P16, the fast way is using the features in CNN as input to the bounding box task. P20</p>
<p>But the problem of different sizes still exists, fast R-CNN uses the max pooling way to ensure the size of the output.</p>
<p>Faster R-CNN: spp-net, improve at the extra pooling layer then fast R-CNN. In this model, the classification, detection, boundary box, etc tasks are all done by different CNNs.</p>
<p>In other words, the nature of the faster here is replacing the original image object detection by the feature object detection. It reduces the computational resources.</p>
<p>Mask R-CNN: deter the pixel in the image along to what label. </p>
<p>RoIAlign is more accurate in the pooling layer, directly dividing the result evenly without considering the number, and using the distance of each point to decide the number of the results after the max pooling.</p>
<p>excepted predict the xyhw, the dx,dy,dh,dw (difference of the xyhw) are also included in the prediction. Jargon called an anchor.</p>
<h1 id="Lecture-12"><a href="#Lecture-12" class="headerlink" title="Lecture 12"></a>Lecture 12</h1><p>high value in Dx, maximize the (1-D(G(z)))</p>
<h1 id="Lecture-13"><a href="#Lecture-13" class="headerlink" title="Lecture 13"></a>Lecture 13</h1><p>diffusion</p>
<p>GAN: 输入为随机输入</p>
<p>f(theta) &#x3D; Y</p>
<p>Y 为图像</p>
<p>X 为随机输入</p>
<p>F（）为神经网络， 映射xy之间的转换关系，从正态分布到图像的分布（特殊的集合）</p>
<p>不用MSE的原因是 噪音和图像之前的关系没有有意义的关系，也就是梯度会没有， 并且只能生成看到过的图片， 不会生成没看见过的图片</p>
<p>生成式对抗模型：两个神经网络，生成式网络（生成假图）和分类器（检验真假）</p>
<p>生成式模型从分类起学会到如何生成真图</p>
<p>分类无法区分的时候，就训练完了</p>
<p>VAE: encoder the image, then decode it</p>
<p>decode之后和原始的 计算loss</p>
<p>从参化</p>
<p>diffusion:  去燥模型，退热定律</p>
<p>X → XT.noisy(高斯)</p>
<p>X → X+Theta → XT.noisy(高斯)</p>
<p>X → … → X+Theta * n → XT.noisy(高斯)</p>
<p>每次加一点点噪音， 直到纯噪音</p>
<p>每次加的高斯分布的噪音</p>
<p>loss 为检测图片中的噪音数量</p>
<p>反过去，就是按照噪音去预测图片</p>
<p>diffusion只有一个NN，其他模型为多个</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-06-16T00:08:50.000Z" title="6/16/2024, 10:08:50 AM">2024-06-16</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-06-20T01:27:25.201Z" title="6/20/2024, 11:27:25 AM">2024-06-20</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/USYD-Lecture/">USYD - Lecture</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2024/06/16/DATA%205207/">USYD - DATA 5207</a></p><div class="content"><h1 id="DATA-5207"><a href="#DATA-5207" class="headerlink" title="DATA 5207"></a>DATA 5207</h1><h1 id="Lecture-2"><a href="#Lecture-2" class="headerlink" title="Lecture 2"></a>Lecture 2</h1><p>presenting data to non-expert (visualization)</p>
<p>less technical knowledge</p>
<p>making data engage</p>
<p>convey the pattern</p>
<p>Data Graphics</p>
<p>3 consideration</p>
<p>what information want to communicate</p>
<p>who is the target audience</p>
<p>why design this feature relevant</p>
<h1 id="Lecture-3"><a href="#Lecture-3" class="headerlink" title="Lecture 3"></a>Lecture 3</h1><p>confounding factors: earning by height, may it occur by gender</p>
<p>select the topic this weekend</p>
<p>RMD template</p>
<h1 id="Lecture-4"><a href="#Lecture-4" class="headerlink" title="Lecture 4"></a>Lecture 4</h1><p>better R square, better job the model does. This means the better-fitting in model</p>
<p>maximize the variables in the model</p>
<p>not only use the technic thing to fit the data but adding the theoretical thing to increase the use in practice</p>
<p>observational data can not make the causal inference (confounding factor included)</p>
<p>model error</p>
<p>random errors (precision limitation - sample number)</p>
<p>systematic error (error in research design - non-sampling error)</p>
<p>difference between observed and actual</p>
<p>response, instrument, interviewer</p>
<p>sample design error</p>
<p>selection, frame</p>
<p>explore dataset  graphs</p>
<p>variables</p>
<p>model choice</p>
<p>correlation plot all variables</p>
<p>variable selection methods  -  stepwise, lasso, or </p>
<h1 id="Lecture-5"><a href="#Lecture-5" class="headerlink" title="Lecture 5"></a>Lecture 5</h1><p>limitation of the LR is assuming the relationship is linear</p>
<p>logits?</p>
<p>ordinal logistic regression (agree, very agree, etc)</p>
<p>For the material in lab 5, the last image can be repainted as for each year, plot the importance of each variable (independent factor) into a single panel.</p>
<h1 id="Lecture-6"><a href="#Lecture-6" class="headerlink" title="Lecture 6"></a>Lecture 6</h1><p>fuzzyjoin is a function that similar operation in SQL</p>
<h2 id="Research-Plan-Format"><a href="#Research-Plan-Format" class="headerlink" title="Research Plan Format"></a>Research Plan Format</h2><h2 id="Format"><a href="#Format" class="headerlink" title="Format"></a>Format</h2><p>Hide the R chunks, the template has the code to hide</p>
<p><strong>key feature should be identified</strong></p>
<p><strong>why use the LR</strong></p>
<h2 id="Literature"><a href="#Literature" class="headerlink" title="Literature"></a>Literature</h2><p>theory from Literature</p>
<p>hypothesis is for testing? is that the previous section provided</p>
<p>literature: tells you, communicate the hypothesis you provide</p>
<p>inform the things you need to do</p>
<p>underpin the thing you want to explain</p>
<p>may error in the literature section, falsify the idea</p>
<h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>api missing</p>
<p>operation</p>
<h2 id="Limitation"><a href="#Limitation" class="headerlink" title="Limitation"></a>Limitation</h2><p>can be deleted</p>
<h1 id="Lecture-7-Quiz-week"><a href="#Lecture-7-Quiz-week" class="headerlink" title="Lecture 7 Quiz week"></a>Lecture 7 Quiz week</h1><p>data can from </p>
<p>consumer data</p>
<p>social media</p>
<p>AB testing (to decide the better version in different versions)</p>
<p>for instance, the color, and size of the button may sent to users, and the amount they click to decide the better version</p>
<p>census</p>
<p>individuals include surveys</p>
<p>web scraping</p>
<h1 id="Lecture-8"><a href="#Lecture-8" class="headerlink" title="Lecture 8"></a>Lecture 8</h1><p>survey</p>
<p>system error - no random</p>
<p>may younger people be more likely to respond to the phone (phone survey) - nonresponse bias</p>
<p>The census is not like the survey, due to its not doing the data sample, according to the entire residents in the country</p>
<p>random error refers to sampling</p>
<h1 id="Assignment-1"><a href="#Assignment-1" class="headerlink" title="Assignment-1"></a>Assignment-1</h1><ol>
<li>Economic: Q288, 50 (income)<ol>
<li>1</li>
</ol>
</li>
<li>Occupation: Q281, 282, 283</li>
<li>Education: Q275<ol>
<li><a target="_blank" rel="noopener" href="https://d1wqtxts1xzle7.cloudfront.net/49101438/18.01.053.20160304-libre.pdf?1474797472=&response-content-disposition=inline;+filename=Effect_of_Education_on_Quality_of_Life_a.pdf&Expires=1713509378&Signature=bTvJ~0cklHa83ixDEhUTW02gYB4KW0iex7Mx6etlJqBNha-f0l-gvirWcVjlpbtaXdn5SsFoSsWtjeay-18z5De6i3e2wRtZvtx5cuzyJe2RLJHKYPPXrkiEORhb9c35JK~-WjFa7T8c8OIQj5RxD11Gj3W7wCsC3jJw~VOewTDYwkVBKXC1-7BjpWcbOSrkZnazJwulzVzLIERo0l6iO51LIqFi6wY8TSiTTdFGhiHctf9bu2Y7IapgVAwDLKXbpYTdXd3c4nVMPqQry~YQ5iOjKVEmcCdMQwn0HUGe837Dn38-7ttCIbNASUOgpjEGQEjmNlznMsOW9jG~X9VHjw__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA">https://d1wqtxts1xzle7.cloudfront.net/49101438/18.01.053.20160304-libre.pdf?1474797472=&response-content-disposition=inline%3B+filename%3DEffect_of_Education_on_Quality_of_Life_a.pdf&amp;Expires&#x3D;1713509378&amp;Signature&#x3D;bTvJ<del>0cklHa83ixDEhUTW02gYB4KW0iex7Mx6etlJqBNha-f0l-gvirWcVjlpbtaXdn5SsFoSsWtjeay-18z5De6i3e2wRtZvtx5cuzyJe2RLJHKYPPXrkiEORhb9c35JK</del>-WjFa7T8c8OIQj5RxD11Gj3W7wCsC3jJw<del>VOewTDYwkVBKXC1-7BjpWcbOSrkZnazJwulzVzLIERo0l6iO51LIqFi6wY8TSiTTdFGhiHctf9bu2Y7IapgVAwDLKXbpYTdXd3c4nVMPqQry</del>YQ5iOjKVEmcCdMQwn0HUGe837Dn38-7ttCIbNASUOgpjEGQEjmNlznMsOW9jG~X9VHjw__&amp;Key-Pair-Id&#x3D;APKAJLOHF5GGSLRBV4ZA</a></li>
<li><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S2214804314001153?via=ihub">https://www.sciencedirect.com/science/article/pii/S2214804314001153?via%3Dihub</a></li>
</ol>
</li>
<li>Societal Wellbeing: Q47 (health)</li>
<li>Security: Q131-138, 52</li>
<li>Social capital, trust: Q57-61<ol>
<li><a target="_blank" rel="noopener" href="https://link.springer.com/article/10.1007/s00148-007-0146-7">https://link.springer.com/article/10.1007/s00148-007-0146-7</a> (neighbourhood only)</li>
</ol>
</li>
</ol>
<p>PCA to combine the multiple variables into one feature</p>
<h1 id="Lecture-10"><a href="#Lecture-10" class="headerlink" title="Lecture 10"></a>Lecture 10</h1><p>cable library in R</p>
<h1 id="Lecture-11-Causality"><a href="#Lecture-11-Causality" class="headerlink" title="Lecture 11 - Causality"></a>Lecture 11 - Causality</h1><h1 id="Lecture-12-Journalism"><a href="#Lecture-12-Journalism" class="headerlink" title="Lecture 12 - Journalism"></a>Lecture 12 - Journalism</h1><p>datasplash platform</p>
<h1 id="Final-Project"><a href="#Final-Project" class="headerlink" title="Final Project"></a>Final Project</h1><p>the plots and tables can be included in the report</p>
<p>using the table to regression result (kable) function</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-12-21T21:06:08.000Z" title="12/22/2023, 8:06:08 AM">2023-12-22</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-06-20T01:34:26.035Z" title="6/20/2024, 11:34:26 AM">2024-06-20</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Pytorch-Introduction/">Pytorch Introduction</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2023/12/22/Tutorial%205/">Pytorch Tutorial 5</a></p><div class="content"><p>The reason use the NN is inner kernel of logistic regression is still linear, to avoid the linear relationship, the NN can use activation function, for instance ReLU.</p>
<p>In this case, we use ReLu as our activation function to predict the image, and it can be found that the accuracy is far better than LR, shows more abilities.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> path, mkdir</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> randint</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.sampler <span class="keyword">import</span> SubsetRandomSampler</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.dataloader <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">dataset = MNIST(root=<span class="string">&quot;./data&quot;</span>, download=<span class="literal">True</span>, transform=ToTensor())</span><br><span class="line">test_dataset = MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=ToTensor())</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">split_indices</span>(<span class="params">n, rate</span>):</span><br><span class="line">    <span class="comment"># create number of validation set</span></span><br><span class="line">    n_val = <span class="built_in">int</span>(n * rate)</span><br><span class="line">    <span class="comment"># create shuffled index from 0-n, with no repeat</span></span><br><span class="line">    idxs = np.random.permutation(n)</span><br><span class="line">    <span class="comment"># retuen (n_val,last) index and (first n_val) index</span></span><br><span class="line">    <span class="comment"># i.e. training index and validation index</span></span><br><span class="line">    <span class="keyword">return</span> idxs[n_val:], idxs[:n_val]</span><br><span class="line"></span><br><span class="line">train_indices, val_indices = split_indices(<span class="built_in">len</span>(dataset), <span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">train_sampler = SubsetRandomSampler(train_indices)</span><br><span class="line">train_loder = DataLoader(dataset,</span><br><span class="line">                         batch_size,</span><br><span class="line">                         sampler=train_sampler)</span><br><span class="line"></span><br><span class="line">val_sampler = SubsetRandomSampler(val_indices)</span><br><span class="line">val_loder = DataLoader(dataset,</span><br><span class="line">                       batch_size,</span><br><span class="line">                       sampler=val_sampler)</span><br><span class="line"></span><br><span class="line">input_size = <span class="number">28</span> * <span class="number">28</span></span><br><span class="line">num_classes = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MnistModel</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_size, hidden_size, out_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.linear1 = nn.Linear(in_size, hidden_size)</span><br><span class="line"></span><br><span class="line">        self.linear2 = nn.Linear(hidden_size, out_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, xb</span>):</span><br><span class="line">        <span class="comment"># flatten</span></span><br><span class="line">        xb = xb.view(xb.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># xb = xb.reshape(xb.size(0), -1)</span></span><br><span class="line">        <span class="keyword">return</span> self.linear2(F.relu(self.linear1(xb)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># for t in model.parameters():</span></span><br><span class="line"><span class="comment">#     print(t.shape)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># for img, labels in train_loder:</span></span><br><span class="line"><span class="comment">#     outputs = model(img)</span></span><br><span class="line"><span class="comment">#     loss = F.cross_entropy(outputs, labels)</span></span><br><span class="line"><span class="comment">#     break</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_device</span>():</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        <span class="keyword">return</span> torch.device(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.device(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">to_device</span>(<span class="params">data, device</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(data, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">        <span class="keyword">return</span> [to_device(x, device) <span class="keyword">for</span> x <span class="keyword">in</span> data]</span><br><span class="line">    <span class="keyword">return</span> data.to(device, non_blocking=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># for img, label in train_loder:</span></span><br><span class="line"><span class="comment">#     print(img.shape)</span></span><br><span class="line"><span class="comment">#     img = to_device(img, device)</span></span><br><span class="line"><span class="comment">#     print(img.device)</span></span><br><span class="line"><span class="comment">#     break</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DeviceDataLoder</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dl, device</span>):</span><br><span class="line">        self.dl = dl</span><br><span class="line">        self.device = device</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># lazy load here</span></span><br><span class="line">        <span class="comment"># instead of load data into device each time, instead, load each batch</span></span><br><span class="line">        <span class="keyword">for</span> b <span class="keyword">in</span> self.dl:</span><br><span class="line">            <span class="keyword">yield</span> to_device(b, self.device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.dl)</span><br><span class="line"></span><br><span class="line"><span class="comment"># use DeviceDataLoader as warpper</span></span><br><span class="line">train_dl = DeviceDataLoder(train_loder, get_device())</span><br><span class="line">valid_dl = DeviceDataLoder(val_loder, get_device())</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss_batch</span>(<span class="params">model, loss_func, xb, yb, opt=<span class="literal">None</span>, metric=<span class="literal">None</span></span>):</span><br><span class="line">    preds = model(xb)</span><br><span class="line"></span><br><span class="line">    loss = loss_func(preds, yb)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">    metric_result = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> metric <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        metric_result = metric(preds, yb)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss.item(), <span class="built_in">len</span>(xb), metric_result</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">model, loss_func, valid_dl, metric=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        results = [loss_batch(model, loss_func, xb, yb, metric=metric)</span><br><span class="line">                   <span class="keyword">for</span> xb, yb <span class="keyword">in</span> valid_dl]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># separate the lists</span></span><br><span class="line">        loss, nums, metric = <span class="built_in">zip</span>(*results)</span><br><span class="line">        total = np.<span class="built_in">sum</span>(nums)</span><br><span class="line">        avg_loss = np.<span class="built_in">sum</span>(np.multiply(loss, nums)) / total</span><br><span class="line">        avg_metric = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> metric <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            avg_metric = np.<span class="built_in">sum</span>(np.multiply(metric, nums)) / total</span><br><span class="line">    <span class="keyword">return</span> avg_loss, total, avg_metric</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">epochs, lr, model, loss_func, train_dl, valid_dl, opt_fn=<span class="literal">None</span>, metric=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">if</span> opt_fn <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        opt_fn = torch.optim.SGD</span><br><span class="line">    opt = opt_fn(model.parameters(), lr=lr)</span><br><span class="line">    loss_history = []</span><br><span class="line">    metric_history = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="keyword">for</span> xb, yb <span class="keyword">in</span> train_dl:</span><br><span class="line">            loss_batch(model, loss_func, xb, yb, opt)</span><br><span class="line">        result = evaluate(model, loss_func, valid_dl, metric)</span><br><span class="line">        val_loss, total, val_metric = result</span><br><span class="line"></span><br><span class="line">        loss_history.append(val_loss)</span><br><span class="line">        metric_history.append(val_metric)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> metric <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>], Loss: <span class="subst">&#123;val_loss:<span class="number">.4</span>f&#125;</span>, Metric: <span class="subst">&#123;val_metric:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>], Loss: <span class="subst">&#123;val_loss:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss_history, metric_history</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">output, label</span>):</span><br><span class="line">    _, preds = torch.<span class="built_in">max</span>(output, dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">sum</span>(label == preds).item() / <span class="built_in">len</span>(preds)</span><br><span class="line"></span><br><span class="line">model = MnistModel(input_size, <span class="number">32</span>, num_classes)</span><br><span class="line">to_device(model, get_device())</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> path.exists(<span class="string">&#x27;./tutorial5/mnist-logistic.pth&#x27;</span>):</span><br><span class="line">    model.load_state_dict(torch.load(<span class="string">&#x27;./tutorial5/mnist-logistic.pth&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    loss_history, metric_history = fit(<span class="number">5</span>, <span class="number">0.5</span>, model, F.cross_entropy,</span><br><span class="line">                                       train_dl,</span><br><span class="line">                                       valid_dl,</span><br><span class="line">                                       opt_fn=torch.optim.SGD,</span><br><span class="line">                                       metric=accuracy)</span><br><span class="line">    <span class="comment"># it will save the weight and bias for this model</span></span><br><span class="line">    <span class="comment"># new dir</span></span><br><span class="line">    mkdir(<span class="string">&#x27;./tutorial5&#x27;</span>)</span><br><span class="line">    torch.save(model.state_dict(), <span class="string">&#x27;./tutorial5/mnist-logistic.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prediction_img</span>(<span class="params">img, model</span>):</span><br><span class="line">    xb = img.unsqueeze(<span class="number">0</span>)</span><br><span class="line">    yb = model(xb)</span><br><span class="line">    _, preds = torch.<span class="built_in">max</span>(yb, dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> preds[<span class="number">0</span>].item()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    img, label = test_dataset[randint(<span class="number">0</span>, <span class="built_in">len</span>(test_dataset) - <span class="number">1</span>)]</span><br><span class="line">    img_np = np.array(img)</span><br><span class="line">    plt.imshow(img_np.squeeze(), cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="built_in">print</span>(prediction_img(img, model))</span><br></pre></td></tr></table></figure></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-12-21T16:42:44.000Z" title="12/22/2023, 3:42:44 AM">2023-12-22</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-06-20T01:29:08.569Z" title="6/20/2024, 11:29:08 AM">2024-06-20</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Pytorch-Introduction/">Pytorch Introduction</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2023/12/22/Tutorial%203/">Pytorch Tutorial 3</a></p><div class="content"><p>simple linear regression with bulit in tools in pytorch</p>
<ol>
<li>generate prediction</li>
<li>calculate the loss</li>
<li>compute gradients of w and b</li>
<li>adjust w and b</li>
<li>reset gradients to zero</li>
</ol>
<p>these 5 steps also respect to the loop in the next function</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># temp, rainfall, humidity</span></span><br><span class="line"><span class="comment"># inputs = torch.tensor(np.random.uniform(0, 120, size=(15, 3)))</span></span><br><span class="line"><span class="comment"># the input and output here need to specify the dtype, otherwise, when torch generate the prediction,</span></span><br><span class="line"><span class="comment"># it will encounter the problem of dtype is not match</span></span><br><span class="line">inputs = torch.tensor(np.array(</span><br><span class="line">    [[<span class="number">109.4144</span>, <span class="number">11.2775</span>, <span class="number">32.4521</span>], [<span class="number">2.0002</span>, <span class="number">47.0248</span>, <span class="number">49.9469</span>], [<span class="number">27.1528</span>, <span class="number">57.8907</span>, <span class="number">91.2076</span>],</span><br><span class="line">     [<span class="number">44.8227</span>, <span class="number">71.6239</span>, <span class="number">64.0752</span>], [<span class="number">66.0968</span>, <span class="number">92.5966</span>, <span class="number">94.0775</span>], [<span class="number">59.6257</span>, <span class="number">76.9701</span>, <span class="number">92.1656</span>],</span><br><span class="line">     [<span class="number">8.1551</span>, <span class="number">1.7426</span>, <span class="number">10.5297</span>], [<span class="number">112.6036</span>, <span class="number">47.2793</span>, <span class="number">95.4221</span>], [<span class="number">3.2212</span>, <span class="number">61.8274</span>, <span class="number">115.9187</span>],</span><br><span class="line">     [<span class="number">35.0351</span>, <span class="number">110.6133</span>, <span class="number">66.6992</span>], [<span class="number">8.8387</span>, <span class="number">21.8008</span>, <span class="number">50.0480</span>], [<span class="number">68.7698</span>, <span class="number">59.9815</span>, <span class="number">12.0230</span>],</span><br><span class="line">     [<span class="number">111.3881</span>, <span class="number">90.3050</span>, <span class="number">62.1327</span>], [<span class="number">101.7462</span>, <span class="number">115.7447</span>, <span class="number">33.4925</span>], [<span class="number">27.7659</span>, <span class="number">54.5803</span>, <span class="number">105.3599</span>]], dtype=<span class="string">&#x27;float32&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># apples, oranges</span></span><br><span class="line"><span class="comment"># targets = torch.tensor(np.random.uniform(0, 50, size=(15, 2)))</span></span><br><span class="line">targets = torch.tensor(np.array(</span><br><span class="line">    [[<span class="number">28.1090</span>, <span class="number">45.0061</span>], [<span class="number">29.0839</span>, <span class="number">6.4205</span>], [<span class="number">35.2633</span>, <span class="number">44.1196</span>],</span><br><span class="line">     [<span class="number">29.5371</span>, <span class="number">6.8457</span>], [<span class="number">7.4298</span>, <span class="number">36.1434</span>], [<span class="number">6.6296</span>, <span class="number">47.1809</span>],</span><br><span class="line">     [<span class="number">49.9750</span>, <span class="number">49.9321</span>], [<span class="number">34.1796</span>, <span class="number">16.6732</span>], [<span class="number">46.8875</span>, <span class="number">7.6084</span>],</span><br><span class="line">     [<span class="number">23.0442</span>, <span class="number">42.2229</span>], [<span class="number">29.7401</span>, <span class="number">13.4199</span>], [<span class="number">3.0854</span>, <span class="number">21.4550</span>],</span><br><span class="line">     [<span class="number">47.6801</span>, <span class="number">49.1518</span>], [<span class="number">18.7320</span>, <span class="number">18.4418</span>], [<span class="number">34.2725</span>, <span class="number">25.8721</span>]], dtype=<span class="string">&#x27;float32&#x27;</span>))</span><br><span class="line"><span class="comment"># print(inputs)</span></span><br><span class="line"><span class="comment"># print(targets)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># TensorDataset will creat the structure of pairing (input and target) accordingly</span></span><br><span class="line">train_ds = TensorDataset(inputs, targets)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">5</span></span><br><span class="line">train_dl = DataLoader(train_ds, batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Each batch size is 5, and the data are shuffled</span></span><br><span class="line"><span class="comment"># and is still can contain the pair of data, the structure won&#x27;t be shuffled</span></span><br><span class="line"><span class="comment"># for xb, yb in train_dl:</span></span><br><span class="line"><span class="comment">#     print(&quot;batch:&quot;)</span></span><br><span class="line"><span class="comment">#     print(xb)</span></span><br><span class="line"><span class="comment">#     print(yb)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># specify the input and output feature number</span></span><br><span class="line">model = nn.Linear(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># the weight and bias will be initialed automatically, and the parameter of requires_grad will be set as True</span></span><br><span class="line"><span class="comment"># print(model.weight)</span></span><br><span class="line"><span class="comment"># print(model.bias)</span></span><br><span class="line"><span class="comment"># print(list(model.parameters()))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># preds = model(inputs)</span></span><br><span class="line"><span class="comment"># print(preds)</span></span><br><span class="line"></span><br><span class="line">loss_fn = F.mse_loss</span><br><span class="line">loss = loss_fn(model(inputs), targets)</span><br><span class="line"><span class="comment"># print(loss)</span></span><br><span class="line"></span><br><span class="line">opt = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 generate prediction</span></span><br><span class="line"><span class="comment"># 2 calculate the loss</span></span><br><span class="line"><span class="comment"># 3 compute gradients of w and b</span></span><br><span class="line"><span class="comment"># 4 adjust w and b</span></span><br><span class="line"><span class="comment"># 5 reset gradients to zero</span></span><br><span class="line"><span class="comment"># these 5 steps also respect to the loop in the next function</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">num_epochs, model, loss_fn, opt</span>):</span><br><span class="line">    <span class="comment"># training interation</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="comment"># batches in each interation</span></span><br><span class="line">        <span class="keyword">for</span> xb, yb <span class="keyword">in</span> train_dl:</span><br><span class="line">            pred = model(xb)</span><br><span class="line">            loss = loss_fn(pred, yb)</span><br><span class="line">            loss.backward()</span><br><span class="line">            opt.step()</span><br><span class="line">            opt.zero_grad()</span><br><span class="line">        <span class="keyword">if</span> (epoch+<span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Epoch [&#123;&#125;/&#123;&#125;], Loss: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch+<span class="number">1</span>, num_epochs, loss.item()))</span><br><span class="line"></span><br><span class="line">fit(<span class="number">100</span>, model, loss_fn, opt)</span><br></pre></td></tr></table></figure></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-12-21T09:46:25.000Z" title="12/21/2023, 8:46:25 PM">2023-12-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-06-20T01:29:10.569Z" title="6/20/2024, 11:29:10 AM">2024-06-20</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Pytorch-Introduction/">Pytorch Introduction</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2023/12/21/Tutorial%204/">Pytorch Tutorial 4</a></p><div class="content"><ol>
<li>load dataset<ol>
<li>transform the data into tensor</li>
</ol>
</li>
<li>split the dataset into training, testing, validation datasets<ol>
<li>define the function of indices shuffle (the dataset are ordered, if missing apply the shuffle, the individual dataset may only contains one label)</li>
<li>create sampler and loader</li>
</ol>
</li>
<li>customise the MnistModel function</li>
<li>define loss_batch<ol>
<li>calculate loss in current batch</li>
</ol>
</li>
<li>define evaluate<ol>
<li>calculate average loss in batches</li>
</ol>
</li>
<li>define accuracy<ol>
<li>also called metric to shows the accuracy</li>
</ol>
</li>
<li>create fit function<ol>
<li>epoch loop<ol>
<li>train loop<ol>
<li>loss_batch  — for train</li>
</ol>
</li>
<li>evaluate result</li>
<li>print result</li>
</ol>
</li>
</ol>
</li>
<li>call fit</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> path</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> randint</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.dataloader <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.sampler <span class="keyword">import</span> SubsetRandomSampler</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># transoforms used to transform the MNIST dataset into tensor in order to torch can work with</span></span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="comment"># here the datasets in original format, can not be understood by torch</span></span><br><span class="line">datasets = MNIST(root=<span class="string">&#x27;./data&#x27;</span>, download=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># print(len(datasets))</span></span><br><span class="line"></span><br><span class="line">test_dataset = MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=transforms.ToTensor())</span><br><span class="line"><span class="comment"># print(len(test_dataset))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># img, label = datasets[0]</span></span><br><span class="line"><span class="comment"># plt.imshow(img, cmap=&#x27;gray&#x27;)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># print(label)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># here the dataset is already transformed into tensor</span></span><br><span class="line">dataset = MNIST(root=<span class="string">&#x27;./data&#x27;</span>, download=<span class="literal">True</span>, transform=transforms.ToTensor())</span><br><span class="line"></span><br><span class="line"><span class="comment"># the shape here is 1,28,28, color, height, weight</span></span><br><span class="line"><span class="comment"># img_tensor, label = dataset[0]</span></span><br><span class="line"><span class="comment"># print(img_tensor.shape, label)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># print(img_tensor[:, 10:15, 10:15])</span></span><br><span class="line"><span class="comment"># print(torch.max(img_tensor), torch.min(img_tensor))</span></span><br><span class="line"><span class="comment"># plt.imshow(img_tensor[0, 10:15, 10:15], cmap=&#x27;gray&#x27;)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">split_indices</span>(<span class="params">n, rate</span>):</span><br><span class="line">    <span class="comment"># create number of validation set</span></span><br><span class="line">    n_val = <span class="built_in">int</span>(n * rate)</span><br><span class="line">    <span class="comment"># create shuffled index from 0-n, with no repeat</span></span><br><span class="line">    idxs = np.random.permutation(n)</span><br><span class="line">    <span class="comment"># retuen (n_val,last) index and (first n_val) index</span></span><br><span class="line">    <span class="comment"># i.e. training index and validation index</span></span><br><span class="line">    <span class="keyword">return</span> idxs[n_val:], idxs[:n_val]</span><br><span class="line"></span><br><span class="line">train_indices, val_indices = split_indices(<span class="built_in">len</span>(dataset), <span class="number">0.2</span>)</span><br><span class="line"><span class="comment"># print(len(train_indices), len(val_indices))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># the sampler here is randomly select the indices from list with number of batch_size</span></span><br><span class="line"><span class="comment"># the reason for this is lower down the training time and computation</span></span><br><span class="line"><span class="comment"># and utilize multiple epoch to train the model, if not, the training will deal with whole data set,</span></span><br><span class="line"><span class="comment"># that will occupy too much memory space and make too much pressure to computational resources.</span></span><br><span class="line"><span class="comment"># in this case, the training process will transfer to smaller chucks</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">train_sampler = SubsetRandomSampler(train_indices)</span><br><span class="line">train_loder = DataLoader(dataset,</span><br><span class="line">                         batch_size,</span><br><span class="line">                         sampler=train_sampler)</span><br><span class="line"></span><br><span class="line">val_sampler = SubsetRandomSampler(val_indices)</span><br><span class="line">val_loder = DataLoader(dataset,</span><br><span class="line">                       batch_size,</span><br><span class="line">                       sampler=val_sampler)</span><br><span class="line"></span><br><span class="line">input_size = <span class="number">28</span> * <span class="number">28</span></span><br><span class="line">num_classes = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># model = nn.Linear(input_size, num_classes)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># print(model.weight.shape)</span></span><br><span class="line"><span class="comment"># print(model.bias.shape)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># print(model.weight)</span></span><br><span class="line"><span class="comment"># print(model.bias)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># for img, label in train_loder:</span></span><br><span class="line"><span class="comment">#     print(img.shape)</span></span><br><span class="line"><span class="comment">#     print(label)</span></span><br><span class="line"><span class="comment">#     # there is a error, the shape of image is 1*28*28, but the received input shape was set 784</span></span><br><span class="line"><span class="comment">#     # so, the customized model are needed.</span></span><br><span class="line"><span class="comment">#     print(model(img))</span></span><br><span class="line"><span class="comment">#     break</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MnistModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># define the input and output for linear</span></span><br><span class="line">        self.linear = nn.Linear(input_size, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, xb</span>):</span><br><span class="line">        <span class="comment"># reshape -1 here avoid the hard code, it will calculate the first dimension number</span></span><br><span class="line">        xb = xb.reshape(-<span class="number">1</span>, input_size)</span><br><span class="line">        <span class="comment"># pass the batch data to linear layer</span></span><br><span class="line">        out = self.linear(xb)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">model = MnistModel()</span><br><span class="line"></span><br><span class="line"><span class="comment"># the weight and bias are in the linear(model.linear.weight), instead of the model above(model.weight)</span></span><br><span class="line"><span class="comment"># print(model.linear.weight.shape)</span></span><br><span class="line"><span class="comment"># print(model.linear.bias.shape)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># print(model.linear.weight)</span></span><br><span class="line"><span class="comment"># print(model.linear.bias)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">l1, l2</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">sum</span>(l1 == l2).item() / <span class="built_in">len</span>(l2)</span><br></pre></td></tr></table></figure>

<p>Log plot presentation</p>
<p><img src="/blog/./post_img/tut4/log.png" alt="Untitled"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># for img, label in train_loder:</span></span><br><span class="line"><span class="comment"># the img pass in the model shape is 100,1,28,28</span></span><br><span class="line"><span class="comment"># the output shape is 100,10</span></span><br><span class="line"><span class="comment"># which reaches what we expected (represent the 0-9 digital number)</span></span><br><span class="line"><span class="comment"># here the softmax can be introduced to show the possibility with each number correspondingly</span></span><br><span class="line"><span class="comment"># possibility = e^y_i / sum(e^y_i)</span></span><br><span class="line"><span class="comment"># outputs = model(img)</span></span><br><span class="line"><span class="comment"># the second parameter here indicates the dim index need to be applied</span></span><br><span class="line"><span class="comment"># so 0 means the column direction, and 1 for row direction for 2D matrix</span></span><br><span class="line"><span class="comment"># probs = F.softmax(outputs, 1)</span></span><br><span class="line"><span class="comment"># print(probs.shape)</span></span><br><span class="line"><span class="comment"># so now the probs shape is 100,10, but each value each row represent possibility(0-1), and sum of each row is 1</span></span><br><span class="line"><span class="comment"># print(outputs.shape)</span></span><br><span class="line"><span class="comment"># print(outputs[0])</span></span><br><span class="line"><span class="comment"># max_probs, predicted_labels = torch.max(probs, 1)</span></span><br><span class="line"><span class="comment"># print(accuracy(predicted_labels, label))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># now, we need to define the loss function</span></span><br><span class="line"><span class="comment"># here the cross entropy is most suitable for logistic regression</span></span><br><span class="line"><span class="comment"># i.e.</span></span><br><span class="line"><span class="comment"># the true label 9 is represented vector of [0,0,0,0,0,0,0,0,0,1]</span></span><br><span class="line"><span class="comment"># the predict vector [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9] for instance</span></span><br><span class="line"><span class="comment"># and the cross entropy is -ln(y*y_pred) i.e. -ln(1*0.9) = 0.10, which is low</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># but, when the prediction is poor</span></span><br><span class="line"><span class="comment"># the true label 1 is represented vector of [0,1,0,0,0,0,0,0,0,0]</span></span><br><span class="line"><span class="comment"># the predict vector [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9] for instance</span></span><br><span class="line"><span class="comment"># and the cross entropy is -ln(y*y_pred) i.e. -ln(1*0.2) = 1.6, which is high</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># in the cross entropy, we only consider the right label, and ignore the other, because their vector is 0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># so when low possibility for the correct number the cross entropy(loss) is high, v.v</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># define the loss function for current batch</span></span><br><span class="line"><span class="comment"># loss = F.cross_entropy(outputs, label)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># the equation here is -e.pow(right prediction possibility)=loss</span></span><br><span class="line"><span class="comment"># so the right possibility is e.pow(-loss)</span></span><br><span class="line"><span class="comment"># learn_rate = 0.001</span></span><br><span class="line"><span class="comment"># optimizer = torch.optim.SGD(model.parameters(), lr=learn_rate)</span></span><br><span class="line"><span class="comment"># optimizer.step()</span></span><br><span class="line"><span class="comment"># break</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss_batch</span>(<span class="params">model, loss_func, xb, yb, opt=<span class="literal">None</span>, metric=<span class="literal">None</span></span>):</span><br><span class="line">    preds = model(xb)</span><br><span class="line">    loss = loss_func(preds, yb)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># metric is used for model evaluation</span></span><br><span class="line">    metric_result = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> metric <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        metric_result = metric(preds, yb)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss.item(), <span class="built_in">len</span>(xb), metric_result</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">model, loss_func, valid_dl, metric=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        results = [loss_batch(model, loss_func, xb, yb, metric=metric)</span><br><span class="line">                   <span class="keyword">for</span> xb, yb <span class="keyword">in</span> valid_dl]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># separate the lists</span></span><br><span class="line">        loss, nums, metric = <span class="built_in">zip</span>(*results)</span><br><span class="line">        total = np.<span class="built_in">sum</span>(nums)</span><br><span class="line">        avg_loss = np.<span class="built_in">sum</span>(np.multiply(loss, nums)) / total</span><br><span class="line">        avg_metric = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> metric <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            avg_metric = np.<span class="built_in">sum</span>(np.multiply(metric, nums)) / total</span><br><span class="line">    <span class="keyword">return</span> avg_loss, total, avg_metric</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">output, label</span>):</span><br><span class="line">    _, preds = torch.<span class="built_in">max</span>(output, dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">sum</span>(label == preds).item() / <span class="built_in">len</span>(preds)</span><br><span class="line"></span><br><span class="line"><span class="comment"># avg_loss, total, val_acc = evaluate(model, F.cross_entropy, val_loder, metric=accuracy)</span></span><br><span class="line"><span class="comment"># print(&quot;Loss: &#123;:.4f&#125;, total:&#123;:.4f&#125;, Accuracy: &#123;:.4f&#125;&quot;.format(avg_loss, total, val_acc))</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">epochs, model, loss_fn, opt, train_dl, valid_dl, metric=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="keyword">for</span> xb, yb <span class="keyword">in</span> train_dl:</span><br><span class="line">            loss, _, _ = loss_batch(model, loss_fn, xb, yb, opt, metric=metric)</span><br><span class="line"></span><br><span class="line">        result = evaluate(model, loss_fn, valid_dl, metric=metric)</span><br><span class="line">        val_loss, total, val_metric = result</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> metric <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Epoch [&#123;&#125;/&#123;&#125;], total:&#123;:.4f&#125;, Loss: &#123;:.4f&#125;&quot;</span></span><br><span class="line">                  .<span class="built_in">format</span>(epoch + <span class="number">1</span>, epochs, total, val_loss, val_metric))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Epoch [&#123;&#125;/&#123;&#125;], total:&#123;:.4f&#125;, Loss: &#123;:.4f&#125;, &#123;&#125;: &#123;:.4f&#125;&quot;</span></span><br><span class="line">                  .<span class="built_in">format</span>(epoch + <span class="number">1</span>, epochs, total, val_loss, metric.__name__, val_metric))</span><br><span class="line"></span><br><span class="line">model = MnistModel()</span><br><span class="line"></span><br><span class="line"><span class="comment"># if path is not blank</span></span><br><span class="line"><span class="keyword">if</span> path.exists(<span class="string">&#x27;mnist-logistic.pth&#x27;</span>):</span><br><span class="line">    model.load_state_dict(torch.load(<span class="string">&#x27;mnist-logistic.pth&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    fit(<span class="number">5</span>,</span><br><span class="line">        model,</span><br><span class="line">        F.cross_entropy,</span><br><span class="line">        torch.optim.SGD(model.parameters(), lr=<span class="number">0.001</span>),</span><br><span class="line">        train_loder,</span><br><span class="line">        val_loder,</span><br><span class="line">        metric=accuracy)</span><br><span class="line">    <span class="comment"># it will save the weight and bias for this model</span></span><br><span class="line">    torch.save(model.state_dict(), <span class="string">&#x27;mnist-logistic.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># read the saved model into instance</span></span><br><span class="line"><span class="comment"># model2 = MnistModel()</span></span><br><span class="line"><span class="comment"># model2.load_state_dict(torch.load(&#x27;mnist-logistic.pth&#x27;))</span></span><br><span class="line"><span class="comment"># model2.state_dict()</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prediction_img</span>(<span class="params">img, model</span>):</span><br><span class="line">    xb = img.unsqueeze(<span class="number">0</span>)</span><br><span class="line">    yb = model(xb)</span><br><span class="line">    _, preds = torch.<span class="built_in">max</span>(yb, dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> preds[<span class="number">0</span>].item()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    img, label = test_dataset[randint(<span class="number">0</span>, <span class="built_in">len</span>(test_dataset) - <span class="number">1</span>)]</span><br><span class="line">    img_np = np.array(img)</span><br><span class="line">    plt.imshow(img_np.squeeze(), cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="built_in">print</span>(prediction_img(img, model))</span><br></pre></td></tr></table></figure>

<h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h2><ol>
<li>when import test_dataset missing the parameter of transform, made the validation section encounter the problem of <code>img no squeeze parameter</code></li>
<li>zip(*results), used for unpack the tuples, and pass into multiple instances</li>
<li><code>avg_loss = np.sum(np.multiply(loss, nums)) / total</code> the reason use multiply here is for last batch number, is might not equals to previous number</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-12-19T15:34:48.000Z" title="12/20/2023, 2:34:48 AM">2023-12-20</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-06-20T01:29:04.720Z" title="6/20/2024, 11:29:04 AM">2024-06-20</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Pytorch-Introduction/">Pytorch Introduction</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2023/12/20/Tutorial%201/">Pytorch Tutorial 1</a></p><div class="content"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">t1 = torch.tensor(<span class="number">4.</span>)</span><br><span class="line"><span class="built_in">print</span>(t1)</span><br><span class="line"><span class="built_in">print</span>(t1.dtype)</span><br><span class="line"></span><br><span class="line">t2 = torch.tensor([<span class="number">1.</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"><span class="built_in">print</span>(t2)</span><br><span class="line"><span class="built_in">print</span>(t2.dtype)</span><br><span class="line"><span class="comment"># in this case the all data will be transformed to same data type</span></span><br><span class="line"><span class="comment"># [1., 2., 3., 4.]</span></span><br><span class="line"></span><br><span class="line">t3 = torch.tensor([<span class="number">1.</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"><span class="built_in">print</span>(t3)</span><br><span class="line"><span class="built_in">print</span>(t3.dtype)</span><br><span class="line"></span><br><span class="line">t4 = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">1.</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">3</span>], [<span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="built_in">print</span>(t4)</span><br><span class="line"><span class="built_in">print</span>(t4.dtype)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(t1.shape)</span><br><span class="line"><span class="built_in">print</span>(t2.shape)</span><br><span class="line"><span class="built_in">print</span>(t3.shape)</span><br><span class="line"><span class="built_in">print</span>(t4.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---</span></span><br><span class="line">x = torch.tensor(<span class="number">3.</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w = torch.tensor(<span class="number">4.</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor(<span class="number">5.</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y = w * x + b</span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line">y.backward()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"><span class="built_in">print</span>(w.grad)</span><br><span class="line"><span class="built_in">print</span>(b.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment">#  convert numpy to torch</span></span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">4</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># use shared memory space, not copy</span></span><br><span class="line">y = torch.from_numpy(x)</span><br><span class="line"></span><br><span class="line"><span class="comment">#  copy data</span></span><br><span class="line">y = torch.tensor(x)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(y.dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># convert torch to numpy</span></span><br><span class="line">z = y.numpy()</span><br><span class="line"><span class="built_in">print</span>(z)</span><br></pre></td></tr></table></figure></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-12-19T15:34:48.000Z" title="12/20/2023, 2:34:48 AM">2023-12-20</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-06-20T01:29:06.834Z" title="6/20/2024, 11:29:06 AM">2024-06-20</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Pytorch-Introduction/">Pytorch Introduction</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2023/12/20/Tutorial%202/">Pytorch Tutorial 2</a></p><div class="content"><p>simple linear regression with auto gradient method in pytorch</p>
<ol>
<li><code>@</code> means inner dot</li>
<li><code>.t()</code> means transpose matrix</li>
<li><code>.numel()</code> means number of element in matrix</li>
<li><code>with torch.no_grad()</code> means code insider this block will not track gradients to save memory and computation time</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">inputs = np.array([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>],</span><br><span class="line">                   [<span class="number">0</span>, <span class="number">1</span>, <span class="number">9</span>],</span><br><span class="line">                   [<span class="number">1</span>, <span class="number">0</span>, <span class="number">8</span>],</span><br><span class="line">                   [<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>]], dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line"></span><br><span class="line">outputs = np.array([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">                    [<span class="number">9</span>, <span class="number">4</span>],</span><br><span class="line">                    [<span class="number">7</span>, <span class="number">3</span>],</span><br><span class="line">                    [<span class="number">6</span>, <span class="number">7</span>]], dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line"></span><br><span class="line">inputs = torch.from_numpy(inputs)</span><br><span class="line">outputs = torch.from_numpy(outputs)</span><br><span class="line"></span><br><span class="line">w = torch.randn(<span class="number">2</span>, <span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.randn(<span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(b)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="comment"># the b is the vector, when the matrix plus b, the b will be copy bunch of data to make it as the matrix</span></span><br><span class="line">    <span class="keyword">return</span> x @ w.t() + b</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mse</span>(<span class="params">t1, t2</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">sum</span>((t1 - t2) ** <span class="number">2</span>) / t1.numel()</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-5</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">    y_pred = model(inputs)</span><br><span class="line">    loss = mse(y_pred, outputs)</span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        w -= learning_rate * w.grad</span><br><span class="line">        b -= learning_rate * b.grad</span><br><span class="line">        w.grad.zero_()</span><br><span class="line">        b.grad.zero_()</span><br><span class="line">    <span class="built_in">print</span>(loss.item())</span><br></pre></td></tr></table></figure></div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/blog/my_img/avatar.png" alt="Quansui"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Quansui</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Sydney, Australia</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/blog/archives"><p class="title">21</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/blog/categories"><p class="title">3</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/blog/tags"><p class="title">25</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/zjsygqs398" target="_blank" rel="noopener">Follow</a></div></div></div><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/blog/categories/Internship/"><span class="level-start"><span class="level-item">Internship</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Pytorch-Introduction/"><span class="level-start"><span class="level-item">Pytorch Introduction</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/USYD-Lecture/"><span class="level-start"><span class="level-item">USYD - Lecture</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-06-16T00:12:25.000Z">2024-06-16</time></p><p class="title"><a href="/blog/2024/06/16/COMP%205046/">USYD - COMP 5046</a></p><p class="categories"><a href="/blog/categories/USYD-Lecture/">USYD - Lecture</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-06-16T00:10:20.000Z">2024-06-16</time></p><p class="title"><a href="/blog/2024/06/16/COMP%205329/">USYD - COMP 5329</a></p><p class="categories"><a href="/blog/categories/USYD-Lecture/">USYD - Lecture</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-06-16T00:08:50.000Z">2024-06-16</time></p><p class="title"><a href="/blog/2024/06/16/DATA%205207/">USYD - DATA 5207</a></p><p class="categories"><a href="/blog/categories/USYD-Lecture/">USYD - Lecture</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-06-15T02:10:42.000Z">2024-06-15</time></p><p class="title"><a href="/blog/2024/06/15/INFO%205992/">USYD - INFO 5992</a></p><p class="categories"><a href="/blog/categories/USYD-Lecture/">USYD - Lecture</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-12-21T21:06:08.000Z">2023-12-22</time></p><p class="title"><a href="/blog/2023/12/22/Tutorial%205/">Pytorch Tutorial 5</a></p><p class="categories"><a href="/blog/categories/Pytorch-Introduction/">Pytorch Introduction</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/blog/archives/2024/"><span class="level-start"><span class="level-item">2024</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/blog/archives/2023/"><span class="level-start"><span class="level-item">2023</span></span><span class="level-end"><span class="level-item tag">15</span></span></a></li><li><a class="level is-mobile" href="/blog/archives/2022/"><span class="level-start"><span class="level-item">2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/blog/tags/Cache/"><span class="tag">Cache</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Data-Analyse/"><span class="tag">Data Analyse</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Deep-Learning/"><span class="tag">Deep Learning</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/GIT/"><span class="tag">GIT</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/IT-Innovation/"><span class="tag">IT Innovation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/JPA/"><span class="tag">JPA</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Java/"><span class="tag">Java</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Linear-Regression/"><span class="tag">Linear Regression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/MQ/"><span class="tag">MQ</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/NLP/"><span class="tag">NLP</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Natural-Language-Processing/"><span class="tag">Natural Language Processing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Python/"><span class="tag">Python</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Pytorch/"><span class="tag">Pytorch</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/R/"><span class="tag">R</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Redis/"><span class="tag">Redis</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Schedule/"><span class="tag">Schedule</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Security/"><span class="tag">Security</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Shiro/"><span class="tag">Shiro</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Social-Science/"><span class="tag">Social Science</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/SpringBoot/"><span class="tag">SpringBoot</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Startups/"><span class="tag">Startups</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Swagger/"><span class="tag">Swagger</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Thread/"><span class="tag">Thread</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Tools/"><span class="tag">Tools</span><span class="tag">2</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/blog/"><img src="/blog/my_img/logo.svg" alt="Hexo" height="28"></a><p class="is-size-7"><span>&copy; 2024 John Doe</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/zjsygqs398"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/blog/js/column.js"></script><script src="/blog/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/blog/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/blog/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/blog/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/blog/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>