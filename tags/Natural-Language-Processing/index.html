<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><title>Tag: Natural Language Processing - Hexo</title><link rel="manifest" href="/blog/manifest.json"><meta name="application-name" content="Quansui&#039;s Blog"><meta name="msapplication-TileImage" content="/my_img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Quansui&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Hexo"><meta property="og:url" content="https://zjsygqs398.github.io/blog"><meta property="og:site_name" content="Hexo"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://zjsygqs398.github.io/blog/img/og_image.png"><meta property="article:author" content="John Doe"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://zjsygqs398.github.io/blog/img/og_image.png"><meta property="fb:app_id"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://zjsygqs398.github.io/blog"},"headline":"Hexo","image":["https://zjsygqs398.github.io/blog/img/og_image.png"],"author":{"@type":"Person","name":"John Doe"},"publisher":{"@type":"Organization","name":"Hexo","logo":{"@type":"ImageObject","url":"https://zjsygqs398.github.io/my_img/logo.svg"}},"description":""}</script><link rel="icon" href="/blog/my_img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/blog/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/blog/"><img src="/blog/my_img/logo.svg" alt="Hexo" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/blog/">Home</a><a class="navbar-item" href="/blog/archives">Archives</a><a class="navbar-item" href="/blog/categories">Categories</a><a class="navbar-item" href="/blog/tags">Tags</a><a class="navbar-item" target="_blank" rel="noopener" href="https://github.com/zjsygqs398">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/zjsygqs398"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/blog/tags">Tags</a></li><li class="is-active"><a href="#" aria-current="page">Natural Language Processing</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-08-01T06:48:00.000Z" title="8/1/2024, 4:48:00 PM">2024-08-01</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-08-01T06:49:00.539Z" title="8/1/2024, 4:49:00 PM">2024-08-01</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/NLP/">NLP</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2024/08/01/Knowledge%20Distillation/">Knowledge Distillation</a></p><div class="content"><h1 id="Knowledge-Distillation"><a href="#Knowledge-Distillation" class="headerlink" title="Knowledge Distillation"></a>Knowledge Distillation</h1><p>article source: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/75031938">https://zhuanlan.zhihu.com/p/75031938</a></p>
<p>The main idea of the knowledge distillation is “forget the models in the ensemble and the way they are parameterised and focus on the function”</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1503.02531">https://arxiv.org/abs/1503.02531</a></p>
<p>在Hinton原文中指出训练时就如同毛毛虫吃树叶来积攒能量，在使用神经网络的时候既做了吃树叶又繁殖的任务，导致效率低下。因此对于这样的问题，希望将一个复杂的模型转变到简单的模型，也就是知识蒸馏做的事情。</p>
<p>蒸馏的过程就是将训练的大模型的内容总结到小模型中，已达到模型复杂度和精度的平衡。在这里会有两个模型，较大的为老师模型，小的模型为学生模型，老师模型通过对于hard targert 进行训练，而学生模型通过老师模型的输出进行收敛。</p>
<p>To achieve training of the student network, the paper mentioned that adding a temperature to scale down the target before input into the softmax.  (i.e., exp(zi&#x2F;T) &#x2F; sum(exp(zi&#x2F;T))) The more larger T in formula, the more smooth distribution the result will be.</p>
<p>The loss function is a * soft + (1-a) * hard, i.e. the trade-off between them.</p>
<p>in general, the larger soft distribution will get better performance in test.</p>
<ol>
<li>训练大模型，也就是使用hard taget， 也就是不对标签做处理</li>
<li>通过老师模型计算soft target</li>
<li>训练小模型<ol>
<li>设置相同T计算结果与soft target 计算loss （from 2）</li>
<li>设置T为1，也就是hard target计算loss</li>
</ol>
</li>
<li>学生模型T设置为1作为最后的预测</li>
</ol>
<p>Summary</p>
<p>The reason knowledge distillation worked is the too sharp distribution of result makes loss function not effect the model, i.e., except the true label, outputs are too close to 0, making the model could not learning anymore. Thus, the introducing of the T to scaling down the result helps to get smooth distribution. In this way, get simper model and better convergence of model.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-06-21T03:22:32.000Z" title="6/21/2024, 1:22:32 PM">2024-06-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-06-21T03:27:20.683Z" title="6/21/2024, 1:27:20 PM">2024-06-21</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/NLP/">NLP</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2024/06/21/SpaCy/">SpaCy</a></p><div class="content"><h1 id="SpaCy"><a href="#SpaCy" class="headerlink" title="SpaCy"></a>SpaCy</h1><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><ol>
<li>It contains the pip installation in python</li>
<li>created a english based nlp object</li>
<li>input some text to create the object of document</li>
<li>operations demo that can get the text from token</li>
<li>span of the token in text</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. installation</span></span><br><span class="line">!pip install spacy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Create a blank English nlp object</span></span><br><span class="line">nlp = spacy.blank(<span class="string">&quot;en&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Created by processing a string of text with the nlp object</span></span><br><span class="line">doc = nlp(<span class="string">&quot;Hello world!&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Index into the Doc to get a single Token</span></span><br><span class="line">token = doc[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the token text via the .text attribute</span></span><br><span class="line"><span class="built_in">print</span>(token.text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterate over tokens in a Doc</span></span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> doc:</span><br><span class="line">    <span class="built_in">print</span>(token.text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. Create another document with text</span></span><br><span class="line">doc = nlp(<span class="string">&quot;Hello NLP class!&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># A slice from the Doc is a Span object</span></span><br><span class="line">span = doc[<span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the span text via the .text attribute</span></span><br><span class="line"><span class="built_in">print</span>(span.text)</span><br></pre></td></tr></table></figure>

<p>demonstrations of <code>is_alpha</code>, <code>is_punct</code>, <code>like_num</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">doc = nlp(<span class="string">&quot;It costs $5.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Index:   &quot;</span>, [token.i <span class="keyword">for</span> token <span class="keyword">in</span> doc])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Text:    &quot;</span>, [token.text <span class="keyword">for</span> token <span class="keyword">in</span> doc])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;is_alpha:&quot;</span>, [token.is_alpha <span class="keyword">for</span> token <span class="keyword">in</span> doc])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;is_punct:&quot;</span>, [token.is_punct <span class="keyword">for</span> token <span class="keyword">in</span> doc])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;like_num:&quot;</span>, [token.like_num <span class="keyword">for</span> token <span class="keyword">in</span> doc])</span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># Index:    [0, 1, 2, 3, 4]</span></span><br><span class="line"><span class="comment"># Text:     [&#x27;It&#x27;, &#x27;costs&#x27;, &#x27;$&#x27;, &#x27;5&#x27;, &#x27;.&#x27;]</span></span><br><span class="line"><span class="comment"># is_alpha: [True, True, False, False, False]</span></span><br><span class="line"><span class="comment"># is_punct: [False, False, False, False, True]</span></span><br><span class="line"><span class="comment"># like_num: [False, False, False, True, False]</span></span><br></pre></td></tr></table></figure>

<h2 id="Part-of-Speech"><a href="#Part-of-Speech" class="headerlink" title="Part of Speech"></a>Part of Speech</h2><p>Load the small english pipeline to achieve the pos</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">!python -m spacy download en_core_web_sm</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the small English pipeline</span></span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Process a text</span></span><br><span class="line">doc = nlp(<span class="string">&quot;She ate the pizza&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterate over the tokens</span></span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> doc:</span><br><span class="line">    <span class="comment"># Print the text and the predicted part-of-speech tag</span></span><br><span class="line">    <span class="built_in">print</span>(token.text, token.pos_, token.pos)</span><br><span class="line"></span><br><span class="line"><span class="comment"># result: </span></span><br><span class="line"><span class="comment"># She PRON 95</span></span><br><span class="line"><span class="comment"># ate VERB 100</span></span><br><span class="line"><span class="comment"># the DET 90</span></span><br><span class="line"><span class="comment"># pizza NOUN 92</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># also the token.head returns the syntactic head token</span></span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> doc:</span><br><span class="line">    <span class="built_in">print</span>(token.text, token.pos_, token.dep_, token.head)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># She PRON nsubj ate</span></span><br><span class="line"><span class="comment"># ate VERB ROOT ate</span></span><br><span class="line"><span class="comment"># the DET det pizza</span></span><br><span class="line"><span class="comment"># pizza NOUN dobj ate</span></span><br></pre></td></tr></table></figure>

<h2 id="Name-Entity-Recognition"><a href="#Name-Entity-Recognition" class="headerlink" title="Name Entity Recognition"></a>Name Entity Recognition</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Process a text</span></span><br><span class="line">doc = nlp(<span class="string">&quot;Apple is looking at buying U.K. startup for $1 billion&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterate over the predicted entities</span></span><br><span class="line"><span class="keyword">for</span> ent <span class="keyword">in</span> doc.ents:</span><br><span class="line">    <span class="comment"># Print the entity text and its label</span></span><br><span class="line">    <span class="built_in">print</span>(ent.text, ent.label_)</span><br></pre></td></tr></table></figure>

<h2 id="Matching"><a href="#Matching" class="headerlink" title="Matching"></a>Matching</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import the Matcher</span></span><br><span class="line"><span class="keyword">from</span> spacy.matcher <span class="keyword">import</span> Matcher</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load a pipeline and create the nlp object</span></span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the matcher with the shared vocab</span></span><br><span class="line">matcher = Matcher(nlp.vocab)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add the pattern to the matcher</span></span><br><span class="line">pattern = [&#123;<span class="string">&quot;TEXT&quot;</span>: <span class="string">&quot;iPhone&quot;</span>&#125;, &#123;<span class="string">&quot;TEXT&quot;</span>: <span class="string">&quot;X&quot;</span>&#125;]</span><br><span class="line">matcher.add(<span class="string">&quot;IPHONE_PATTERN&quot;</span>, [pattern])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Process some text</span></span><br><span class="line">doc = nlp(<span class="string">&quot;iPhone X news! Upcoming iPhone X release date leaked&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Call the matcher on the doc</span></span><br><span class="line">matches = matcher(doc)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterate over the matches</span></span><br><span class="line"><span class="keyword">for</span> pattern_id, start, end <span class="keyword">in</span> matches:</span><br><span class="line">    <span class="comment"># Get the matched span</span></span><br><span class="line">    matched_span = doc[start:end]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;&quot;&#123;&#125;&quot; - match for pattern &#123;&#125; in span (&#123;&#125;, &#123;&#125;)&#x27;</span>.<span class="built_in">format</span>(matched_span.text, pattern_id, start, end))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># &quot;iPhone X&quot; - match for pattern 9528407286733565721 in span (0, 2)</span></span><br><span class="line"><span class="comment"># &quot;iPhone X&quot; - match for pattern 9528407286733565721 in span (5, 7)</span></span><br></pre></td></tr></table></figure>

<p>in match pattern, LEMMA contains the multiple variations of the word</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">pattern = [</span><br><span class="line">    &#123;<span class="string">&quot;LEMMA&quot;</span>: <span class="string">&quot;love&quot;</span>, <span class="string">&quot;POS&quot;</span>: <span class="string">&quot;VERB&quot;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;POS&quot;</span>: <span class="string">&quot;NOUN&quot;</span>&#125;</span><br><span class="line">]</span><br><span class="line">matcher = Matcher(nlp.vocab)</span><br><span class="line">matcher.add(<span class="string">&quot;LOVE_PATTERN&quot;</span>, [pattern])</span><br><span class="line"></span><br><span class="line">doc = nlp(<span class="string">&quot;I loved vanilla but now I love chocolate more.&quot;</span>)</span><br><span class="line"></span><br><span class="line">matches = matcher(doc)</span><br><span class="line"><span class="keyword">for</span> pattern_id, start, end <span class="keyword">in</span> matches:</span><br><span class="line">    matched_span = doc[start:end]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;&quot;&#123;&#125;&quot; - match for pattern &#123;&#125; in span (&#123;&#125;, &#123;&#125;)&#x27;</span>.<span class="built_in">format</span>(matched_span.text, pattern_id, start, end))</span><br><span class="line"></span><br><span class="line"><span class="comment"># &quot;loved vanilla&quot; - match for pattern 4358456325055851256 in span (1, 3)</span></span><br><span class="line"><span class="comment"># &quot;love chocolate&quot; - match for pattern 4358456325055851256 in span (6, 8)</span></span><br></pre></td></tr></table></figure>

<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Operators and quantifiers let you define how often a token should be matched. They can be added using the &quot;OP&quot; key. &quot;OP&quot; can have one of four values:</span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> An &quot;!&quot; negates the token, so it&#x27;s matched 0 times.</span><br><span class="line"><span class="bullet">-</span> A &quot;?&quot; makes the token optional, and matches it 0 or 1 times.</span><br><span class="line"><span class="bullet">-</span> A &quot;+&quot; matches a token 1 or more times.</span><br><span class="line"><span class="bullet">-</span> And finally, an &quot;<span class="emphasis">*&quot; matches 0 or more times.</span></span><br></pre></td></tr></table></figure>

<h2 id="Word-Vector"><a href="#Word-Vector" class="headerlink" title="Word Vector"></a>Word Vector</h2><p>SpaCy contains the word vector in medium pipeline</p>
<p>It could output the word vectors and calculate the similarity of tokens </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load a larger pipeline with vectors</span></span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_md&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Look at one word vector</span></span><br><span class="line">doc = nlp(<span class="string">&quot;I love chocolate&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(doc[<span class="number">2</span>].vector)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compare two documents</span></span><br><span class="line">doc1 = nlp(<span class="string">&quot;I like fast food&quot;</span>)</span><br><span class="line">doc2 = nlp(<span class="string">&quot;I like pizza&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Comparing sentences:&quot;</span>, doc1.similarity(doc2))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compare two tokens</span></span><br><span class="line">doc = nlp(<span class="string">&quot;I like pizza and pasta&quot;</span>)</span><br><span class="line">token1 = doc[<span class="number">2</span>]</span><br><span class="line">token2 = doc[<span class="number">4</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Comparing &#x27;pizza&#x27; and &#x27;paste&#x27;:&quot;</span>, token1.similarity(token2))</span><br></pre></td></tr></table></figure>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li><a target="_blank" rel="noopener" href="https://course.spacy.io/en/">https://course.spacy.io/en/</a></li>
<li><a target="_blank" rel="noopener" href="https://spacy.io/">https://spacy.io/</a></li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-06-16T00:12:25.000Z" title="6/16/2024, 10:12:25 AM">2024-06-16</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-06-20T03:50:03.795Z" title="6/20/2024, 1:50:03 PM">2024-06-20</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/USYD-Lecture/">USYD - Lecture</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2024/06/16/COMP%205046/">USYD - COMP 5046</a></p><div class="content"><h1 id="Lecture-1"><a href="#Lecture-1" class="headerlink" title="Lecture 1"></a>Lecture 1</h1><p>N-Gram LMs</p>
<p>The 4 ways to deal with the unseen word sequence</p>
<p>smoothing:</p>
<p><img src="https://zjsygqs398.github.io/blog/post_img/COMP%205046/Untitled.png" alt="Untitled"></p>
<p>discounting:</p>
<p><img src="https://zjsygqs398.github.io/blog/post_img/COMP%205046/Untitled%201.png" alt="Untitled"></p>
<p>Interpolation: using the multiple N-Gram probability </p>
<p><img src="https://zjsygqs398.github.io/blog/post_img/COMP%205046/Untitled%202.png" alt="Untitled"></p>
<p>Kneser-Ney smoothing:</p>
<h1 id="Lecture2"><a href="#Lecture2" class="headerlink" title="Lecture2"></a>Lecture2</h1><h1 id="Lecture3"><a href="#Lecture3" class="headerlink" title="Lecture3"></a>Lecture3</h1><h1 id="Lecture4"><a href="#Lecture4" class="headerlink" title="Lecture4"></a>Lecture4</h1><p>Note the formula macro and micro F1 score — ass2</p>
<p>Not the find_best_socre initial number should be negative — ass2</p>
<p>directly using average will make some features vanish (+ and -), also the order is meaningless</p>
<p>in slide, NER page 66</p>
<p>labels number * length * length means:</p>
<p>each token in sequence is possible for start or end, that is length * length</p>
<p>then, each span can be predicted as each label</p>
<p>so, the output should be (labels * length * length)</p>
<p><img src="https://zjsygqs398.github.io/blog/post_img/COMP%205046/Untitled%203.png" alt="Untitled"></p>
<p><img src="https://zjsygqs398.github.io/blog/post_img/COMP%205046/Untitled%204.png" alt="Untitled"></p>
<h1 id="Lecture-5"><a href="#Lecture-5" class="headerlink" title="Lecture 5"></a>Lecture 5</h1><p>beam search</p>
<p>in nature, greedy search is to find the local optimal solution in each step</p>
<h1 id="Lecture-6"><a href="#Lecture-6" class="headerlink" title="Lecture 6"></a>Lecture 6</h1><p>the label in current word from LSTM purely, B I O possibility(sum as 1)</p>
<p>score &#x3D; current label p * best (previous p * transfer p)</p>
<p>it from </p>
<p>score &#x3D; current label p * best (i.e. DP * transfer model)</p>
<p>DP refers to the optimal route in choice</p>
<p>transfer model refers to the Markov model</p>
<p>co reference</p>
<p>dependency parsing</p>
<h1 id="Lecture-7"><a href="#Lecture-7" class="headerlink" title="Lecture 7"></a>Lecture 7</h1><h1 id="Lecture-8"><a href="#Lecture-8" class="headerlink" title="Lecture 8"></a>Lecture 8</h1><p>disadvantages of self-attention</p>
<p><strong>problem:</strong></p>
<p>non-linear lacking</p>
<p>order lacking</p>
<p><strong>resolve:</strong></p>
<p>feedforward layer</p>
<p>additional position vector</p>
<p>issue: the length of the position vector is fixed, not flexible enough<br>the mask is for ignoring the future word in the training process, it lets the model know what is known in this step, and what is the prediction in this step. In math, put the infinite negative value for future words and dot the product in the current word<br>the multiple layers of self-attention are used to build better representation of the word</p>
<p>query: curve line </p>
<p>key: embedding word</p>
<p>value: passing into the weighted value</p>
<h1 id="Lecture-10"><a href="#Lecture-10" class="headerlink" title="Lecture 10"></a>Lecture 10</h1><p>data can from </p>
<p>fineweb</p>
<p>common craw</p>
<h1 id="Lecture-11"><a href="#Lecture-11" class="headerlink" title="Lecture 11"></a>Lecture 11</h1><h1 id="Review"><a href="#Review" class="headerlink" title="Review"></a>Review</h1><p>The content below is the missing slide week.</p>
<p>parsing, 句法分析, identity the grammar structure by sub-phrases</p>
<p>The span in the sentence called the phrase</p>
<p>treebanks</p>
<p>Method: Dependency Parsing &#x2F; Grammar</p>
<p>compute every possible dependency to find the best score</p>
<p>Or, choose a way to get the best overall score</p>
<p>co-reference</p>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/blog/my_img/avatar.png" alt="Quansui"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Quansui</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Sydney, Australia</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/blog/archives"><p class="title">23</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/blog/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/blog/tags"><p class="title">26</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/zjsygqs398" target="_blank" rel="noopener">Follow</a></div></div></div><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/blog/categories/Internship/"><span class="level-start"><span class="level-item">Internship</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/Pytorch-Introduction/"><span class="level-start"><span class="level-item">Pytorch Introduction</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/blog/categories/USYD-Lecture/"><span class="level-start"><span class="level-item">USYD - Lecture</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-08-01T06:48:00.000Z">2024-08-01</time></p><p class="title"><a href="/blog/2024/08/01/Knowledge%20Distillation/">Knowledge Distillation</a></p><p class="categories"><a href="/blog/categories/NLP/">NLP</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-06-21T03:22:32.000Z">2024-06-21</time></p><p class="title"><a href="/blog/2024/06/21/SpaCy/">SpaCy</a></p><p class="categories"><a href="/blog/categories/NLP/">NLP</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-06-16T00:12:25.000Z">2024-06-16</time></p><p class="title"><a href="/blog/2024/06/16/COMP%205046/">USYD - COMP 5046</a></p><p class="categories"><a href="/blog/categories/USYD-Lecture/">USYD - Lecture</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-06-16T00:10:20.000Z">2024-06-16</time></p><p class="title"><a href="/blog/2024/06/16/COMP%205329/">USYD - COMP 5329</a></p><p class="categories"><a href="/blog/categories/USYD-Lecture/">USYD - Lecture</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-06-16T00:08:50.000Z">2024-06-16</time></p><p class="title"><a href="/blog/2024/06/16/DATA%205207/">USYD - DATA 5207</a></p><p class="categories"><a href="/blog/categories/USYD-Lecture/">USYD - Lecture</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/blog/archives/2024/"><span class="level-start"><span class="level-item">2024</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/blog/archives/2023/"><span class="level-start"><span class="level-item">2023</span></span><span class="level-end"><span class="level-item tag">15</span></span></a></li><li><a class="level is-mobile" href="/blog/archives/2022/"><span class="level-start"><span class="level-item">2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/blog/tags/Cache/"><span class="tag">Cache</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Data-Analyse/"><span class="tag">Data Analyse</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Deep-Learning/"><span class="tag">Deep Learning</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/GIT/"><span class="tag">GIT</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/IT-Innovation/"><span class="tag">IT Innovation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/JPA/"><span class="tag">JPA</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Java/"><span class="tag">Java</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Linear-Regression/"><span class="tag">Linear Regression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/MQ/"><span class="tag">MQ</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/NLP/"><span class="tag">NLP</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Natural-Language-Processing/"><span class="tag">Natural Language Processing</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Python/"><span class="tag">Python</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Pytorch/"><span class="tag">Pytorch</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/R/"><span class="tag">R</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Redis/"><span class="tag">Redis</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Schedule/"><span class="tag">Schedule</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Security/"><span class="tag">Security</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Shiro/"><span class="tag">Shiro</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Social-Science/"><span class="tag">Social Science</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/SpaCy/"><span class="tag">SpaCy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/SpringBoot/"><span class="tag">SpringBoot</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Startups/"><span class="tag">Startups</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Swagger/"><span class="tag">Swagger</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Thread/"><span class="tag">Thread</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Tools/"><span class="tag">Tools</span><span class="tag">2</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/blog/"><img src="/blog/my_img/logo.svg" alt="Hexo" height="28"></a><p class="is-size-7"><span>&copy; 2024 John Doe</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/zjsygqs398"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/blog/js/column.js"></script><script src="/blog/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/blog/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/blog/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/blog/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/blog/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>